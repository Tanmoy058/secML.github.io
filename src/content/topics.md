This page collects some topic ideas and papers for future classes.
These are just suggestions, not meant to be an exhaustive list or limit the scope of future topics or papers.

### Certified Defenses, Formal Methods

Aditi Raghunathan, Jacob Steinhardt, Percy Liang. _Certified Defenses against Adversarial Examples_. [PDF](https://arxiv.org/abs/1801.09344)

Guy Katz, Clark Barrett, David Dill, Kyle Julian and Mykel Kochenderfer. _Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks_. [PDF](https://arxiv.org/pdf/1702.01135.pdf)

Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. _Ground-Truth Adversarial Examples_. [PDF](https://arxiv.org/pdf/1709.10207.pdf)

J. Zico Kolter, Eric Wong. _Provable defenses against adversarial examples via the convex outer adversarial polytope_. [PDF](https://arxiv.org/abs/1711.00851)

### Testing

Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana. _DeepXplore: Automated Whitebox Testing of Deep Learning Systems_. [PDF](https://arxiv.org/abs/1705.06640)

### Defining Adversarial Examples

Mahmood Sharif, Lujo Bauer, Michael K. Reiter. _On the Suitability of \\(L_p\\)-norms for Creating and Preventing Adversarial Examples_. arxiv, Feb 2018. [PDF](https://arxiv.org/abs/1802.09653)

Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein. _Adversarial Examples that Fool both Human and Computer Vision_. arxiv, Feb 2018. [PDF](https://arxiv.org/abs/1802.08195)

### Adversarial Training

Alex Kurakin, Dan Boneh, Florian Tram√®r, Ian Goodfellow, Nicolas Papernot, Patrick McDaniel. _Ensemble Adversarial Training: Attacks and Defenses_. ICLR 2018. [Web](https://research.google.com/pubs/pub46638.html)

### Malware Detection and Evasion

Robin Sommer and Vern Paxson. _Outside the Closed World: On Using Machine Learning For Network Intrusion Detection_. [PDF](https://oaklandsok.github.io/papers/sommer2010.pdf)

Igino Corona and Giorgio Giacinto and Fabio Roli. _Adversarial Attacks against Intrusion Detection
Systems: Taxonomy, Solutions and Open Issues_. [PDF](http://pralab.diee.unica.it/sites/default/files/Corona-INS2013.pdf)

Kyle Soska and Nicolas Christin. _Automatically Detecting Vulnerable Websites Before They Turn Malicious_. [PDF](https://www.andrew.cmu.edu/user/nicolasc/publications/SC-USENIXSec14.pdf)

Roberto Jordaney, Kumar Sharad, Santanu K. Dash, Zhi Wang, Davide Papini, Ilia Nouretdinov, and Lorenzo Cavallaro. _Transcend: Detecting Concept Drift in Malware Classification Models_. [Paper Site](https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney)

Hung Dang, Yue Huang, and Ee-Chien Chang. _Evading Classifiers by Morphing in the Dark_. [PDF](https://acmccs.github.io/papers/p119-dangA.pdf)

### Fairness

Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan. _Semantics derived automatically from language corpora contain human-like biases_. [HTML](http://science.sciencemag.org/content/356/6334/183.full)

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. _Men Also Like Shopping:
Reducing Gender Bias Amplification using Corpus-level Constraints_. [PDF](https://arxiv.org/pdf/1707.09457.pdf)

[Conference on Fairness, Accountability, and 
Transparency](https://fatconference.org/2018/program.html)

### Assessing Risks

_The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation_. February 2018
[PDF](https://www.eff.org/files/2018/02/20/malicious_ai_report_final.pdf)

### Poisoning

Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and
Fabio Roli. _Bagging Classifiers for Fighting Poisoning Attacks in
Adversarial Classification
Tasks_. [PDF](http://192.167.131.140/sites/default/files/biggio11-mcs.pdf)

Scott Alfeld, Xiaojin Zhu, and Paul Barford.  _Data Poisoning Attacks
against Autoregressive
Models_. [PDF](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12049/11758)
