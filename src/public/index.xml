<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>secML</title>
    <link>https://secml.github.io/index.xml</link>
    <description>Recent content on secML</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://secml.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Class 6: Measuring Robustness of ML Models</title>
      <link>https://secml.github.io/class6/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class6/</guid>
      <description>

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;In what seems to be an endless back-and-forth between new adversarial attacks and new defenses against those attacks, we would like a means of formally verifying the robustness of machine learning algorithms to adversarial attacks.
 In the privacy domain, there is the idea of a differential privacy budget, which quantifies privacy over all possible attacks. In the following three papers, we see attempts at deriving an equivalent benchmark for security, one that will allow the evaluation of defenses against all possible attacks instead of just a specific one.&lt;/p&gt;

&lt;h2 id=&#34;provably-minimally-distorted-adversarial-examples&#34;&gt;Provably Minimally Distorted Adversarial Examples&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Nicholas Carlini, Guy Katz, Clark Barrett, David L. Dill. &lt;em&gt;Provably Minimally-Distorted Adversarial Examples&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1709.10207&#34;&gt;arXiv preprint arXiv:1709.10207&lt;/a&gt;. September 2017.&lt;/p&gt;

&lt;p&gt;Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer. &lt;em&gt;Reluplex: An efficient SMT solver for verifying deep neural networks&lt;/em&gt;. International Conference on Computer Aided Verification. 2017. &lt;a href=&#34;https://arxiv.org/pdf/1702.01135.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There have been many attacking techniques against deep learning models that can effectively generate adversarial examples, such as FGSM, JSMA, DeepFool and the Carlini &amp;amp; Wagner attacks. But most of them couldn&amp;rsquo;t verify the absence of adversarial examples even if they fail to find any result.&lt;/p&gt;

&lt;p&gt;Researchers have started to borrow the idea and techniques from the program verification field to verify the robustness of neural networks. In order to verify the correctness of a program, we can encode a program into SAT-modulo-theories (SMT) formulas and use some off-the-shelf solvers (e.g. Microsoft Z3) to verify a correctness property. An SMT solver generates sound and complete results, either telling you the program never violates the property, or giving you some specific counter-examples. However, we may not be able to get the results for some large programs in our lifetime, because it is an NP-complete problem.&lt;/p&gt;

&lt;p&gt;Similarly, the current neural network verification techniques haven&amp;rsquo;t been able to deal with deep learning models of arbitrary size. But one of the prototype named Reluplex has produced some promising results on the MNIST dataset.&lt;/p&gt;

&lt;p&gt;The Reluplex is an extension of the Simplex algorithm. It introduces a new domain theory solver to take care of the ReLU activation function because the Simplex only deals with linear real arithmetic. You can find more details about Reluplex in:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer. &lt;em&gt;Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1702.01135&#34;&gt;Arxiv&lt;/a&gt;. 2017.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The paper we discuss here uses Reluplex to verify the &lt;em&gt;local adversarial robustness&lt;/em&gt; of two MNIST classification models. We say a model is delta-locally-robust at input \(x\) if for every \(x&amp;rsquo;\) such that \( ||x-x&amp;rsquo;||p \le \delta \), the model predicts the same label for \(x\) and \(x&amp;rsquo;\). Local robustness is certified for individual inputs, which is substantially different from the &lt;em&gt;global robustness&lt;/em&gt; that certifies the whole input space. The paper performs binary search on delta to find the minimal distortion at a certain precision, and each delta corresponds to one execution instance of Reluplex. The paper only considers the \(L_{infinity}\) norm and the \(L_1\) norm because it is easier to encode these constraints with Reluplex. For example, the \(L_1\) norm could be encoded as
\( |x| = max(x, -x)=ReLu(2x)-x \).&lt;/p&gt;

&lt;p&gt;The evaluation is conducted on a fully-connected, 3-layer network that has 20k weights and fewer than 100 hidden neurons. The testing accuracy of the model is 97%. The model is smaller than most of the state-of-the-art models and has inferior accuracy, but should be good enough for the model verification prototype. The authors arbitrarily selected 10 source images with known labels from the MNIST test set, which produced 90 targeted attack instances in total.&lt;/p&gt;

&lt;p&gt;Even though the Reluplex method is faster than most of the existing general SMT solvers, it is not fast enough for verifying the MNIST classification models. For every configuration with different target models and different \(L_p\) norm constraints, Reluplex always timed out for some of the 90 instances. The experiments with the \(L_1\) constraint were generally slower than those with the
L_infinity constraint, because it introduced more ReLU components. However, we still found some interesting results from the successful verification instances.&lt;/p&gt;

&lt;p&gt;The paper compared the minimally-distorted adversarial examples found by Reluplex with those generated by the CW attack and concluded that iterative optimization-based attacks are effective because the CW attack produced adversarial examples within 12% of optimal on the specific models.&lt;/p&gt;

&lt;p&gt;The paper also evaluated a particular defense technique proposed by Madry et al. which is an adversarial training method that uses the PGD attack and enlarges the model capacity. The paper concluded that the PGD-based adversarial training increased the robustness to adversarial examples by 4.2x on the examined samples. Even though this result doesn&amp;rsquo;t guarantee the efficacy at larger scale, it proves that the defense increases the robustness against all future attacks while it is only trained with the PGD attack.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;evaluating-the-robustness-of-neural-networks&#34;&gt;Evaluating the Robustness of Neural Networks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, Luca Daniel. &lt;em&gt;Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach&lt;/em&gt;. ICLR 2018. January 2018 &lt;a href=&#34;https://arxiv.org/pdf/1801.10578.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Little work has been done towards developing a comprehensive measure of robustness for neural networks, primarily due to them growing mathematically complex as the number of layers increases. The authors contribute a lower bound on the minimal perturbation needed to generate an adversarial sample. They accomplish this by using the extreme value theorem to estimate the local Lipschitz constant for a given sample.&lt;/p&gt;

&lt;p&gt;The work is motivated by the success of a particular attack by Carlini and Wagner against several previous defense strategies such as defensive distillation, adversarial training, and model ensemble &lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;[2]&lt;/a&gt;. This highlights the need for a means for evaluating a defenses effectiveness against &lt;em&gt;all&lt;/em&gt; attacks rather than just the ones tested against.&lt;/p&gt;

&lt;p&gt;Previous attempts at deriving such a lower bound have shortcomings of their own. Szegedy et al. compute the product of the global Lipschitz constant of each layer of the network to derive a metric of instability of a deep neural network&amp;rsquo;s output; however the global Lipschitz constant is a loose bound &lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hein and Andriushchenko derived a closed-form bound using the local Lipschitz constant, but such a bound is only feasible for networks with one hidden layer. Several other approaches used linear programming to verify properties of neural networks, but they are also infeasible for large networks &lt;a href=&#34;https://arxiv.org/pdf/1705.08475.pdf&#34;&gt;[4]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The most similar work uses a linear programming formulation to derive an &lt;em&gt;upper&lt;/em&gt; bound on the minimum distortion needed, which is not as useful as a universal robustness metric, and also is infeasible for large networks due to the computational complexity of linear programming.&lt;/p&gt;

&lt;h3 id=&#34;robustness-metric&#34;&gt;Robustness metric&lt;/h3&gt;

&lt;p&gt;In order to derive a formal robustness guarantee, the authors formulate the lower bound for the minimum adversarial distortion needed to be a successful perturbed example (success here meaning fooling the classifier, thus becoming adversarial). They accomplish this by relating p-norm of the perturbation \(\lVert \delta \rVert_p \) to the local Lipschitz constant \( L_q^j \), local meaning defined over a l-ball around the input
They use properties of Lipschitz continuous functions to prove the following:&lt;/p&gt;

&lt;p&gt;$$ \lVert \delta \rVert \le \min_{j \ne c}  \frac{f_c(x_0) - f_j(x_0)}{L_q^j} $$&lt;/p&gt;

&lt;p&gt;That is to say that if the the p-norm of the perturbation is less than the difference between any two classifications of the input, \((x_0\)), divided by the local Lipschitz constant.&lt;/p&gt;

&lt;p&gt;The authors go on to provide a similar guarantee for networks where the activation function is not differentiable, for example an ReLU network. In such a case, the local Lipschitz constant can be replaced with the supremum of the directional derivatives for each direction heading towards the non-differentiable point.&lt;/p&gt;

&lt;h3 id=&#34;clever-robustness-metric&#34;&gt;CLEVER Robustness Metric&lt;/h3&gt;

&lt;p&gt;Since the above formulations of the lower bound are difficult to compute, the authors present a technique for estimating the lower bound, which is much more feasible computationally. They make use of the extreme value theorem, which claims that for any random variable, the maximum value of infinite samples of it follows a known distribution. In this case, the random variable is the p-norm of the gradient of a given sample. The authors assume a Weibull non-degenerate distribution for this paper and verify it as a reasonable assumption empirically.&lt;/p&gt;

&lt;p&gt;To apply the theorem, they generate N samples in a ball around a given sample and calculate the gradient norm of each sample. Using the maximum value of the gradient over those N samples, they apply maximum likelihood to get distribution parameters that maximize the probability of those gradients. It should be noted that this approach assumes that the adversarial examples are well-distributed enough that enough random noise will generate them.&lt;/p&gt;

&lt;p&gt;The resulting CLEVER score is an approximate lower bound on the distortion needed for an attack to succeed.&lt;/p&gt;

&lt;h3 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h3&gt;

&lt;p&gt;To evaluate how effective an indicator of robustness the CLEVER score is, the authors conducted experiments using the CIFAR-10, MNIST, and ImageNet data sets, pairing each data set with a set of neural network architectures and corresponding popular defenses for those networks.&lt;/p&gt;

&lt;p&gt;They estimate the Weibull distribution parameter and conduct a goodness-of-fit test to verify that the distribution fits the data empirically.&lt;/p&gt;

&lt;p&gt;They then apply two recent state-of-the-art attacks to their models, the Carlini and Wagner attack &lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;[2]&lt;/a&gt;, covered in a previous blog post, and I-FGSM &lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To evaluate the CLEVER score&amp;rsquo;s effectiveness, the authors compare it with the average \( L_2 \) and
L_infinity distortions for each adversarial example generated by each type of attack. Since the score is an estimate of the lower bound of the minimal distortion needed, if it is an accurate estimate, then no attack should succeed with a distortion less than the lower bound. Their results show that this holds under both distortion metrics.&lt;/p&gt;

&lt;p&gt;These results also show that the Carlini Wagner attack produces adversarial examples with distortion much closer to minimal than I-FGSM, which demonstrates the CLEVER score&amp;rsquo;s ability to evaluate the strength of attacks themselves. The score also serves as a metric for the effectiveness of of defenses against adversarial attacks since the score was shown to increase for defensively distilled networks. The authors generally contribute a means of providing theoretical guarantees about neural networks that is no limited by the size of the network.&lt;/p&gt;

&lt;p&gt;However, there does not seem to be a correlation between the CLEVER score and the distortion needed by the Carlini Wagner attack. If the score were to truly indicate how hard it is to generate adversarial examples, then we would expect that networks with a higher CLEVER score to have higher average distortions than networks with lower scores, but this was not always the case.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;lower-bounds-on-the-robustness-to-adversarial-perturbations&#34;&gt;Lower bounds on the Robustness to Adversarial Perturbations&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Jonathan Peck, Joris Roels, Bart Goossens, Yvan Saeys &lt;a href=&#34;https://papers.nips.cc/paper/6682-lower-bounds-on-the-robustness-to-adversarial-perturbations.pdf&#34;&gt;Lower bounds on the robustness to adversarial perturbations&lt;/a&gt;. NIPS 2017.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this paper, the authors propose theoretical lower bounds on the adversarial
perturbations on different types of layers in neural networks. They then combine
these theoretical lower bounds derived layer-by-layer, and apply it to the
entire network to calculate the theoretical lower bound for adversarial
perturbations for the network. In contrast to previous work on this topic, the
authors derive the lower bounds directly in terms of the model parameters.
This is useful for applying the bounds on real-world DNN models.&lt;/p&gt;

&lt;h3 id=&#34;lower-bounds-on-different-classifiers&#34;&gt;Lower bounds on different classifiers&lt;/h3&gt;

&lt;p&gt;The authors use a modular approach to find the lower bound of a feedforward
neural network. In this approach, they derive the lower bound at a particular
layer by working their way backward starting from the output layer, and
gradually towards the input layer. More precisely, given a layer that takes as
input \(y\) and computes a function \(h\) on the input, if we know the
robustness bound of the following layer \(k\), then the goal at the current
layer is to find the perturbation \(r\) such that,&lt;/p&gt;

&lt;p&gt;$$||h(y+r)|| = ||h(y)|| + k$$&lt;/p&gt;

&lt;p&gt;Any adversarial perturbation to that layer, \(q\) is guaranteed to satisfy
\(||q|| \geq ||r||\).&lt;/p&gt;

&lt;p&gt;The lower bounds for different types of layers, as proposed in the paper, is
shown below. The proofs can be found in the &lt;a href=&#34;https://papers.nips.cc/paper/6682-lower-bounds-on-the-robustness-to-adversarial-perturbations-supplemental.zip&#34;&gt;supplemental materials provided by
the authors of the paper&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;softmax-output-layers&#34;&gt;Softmax output layers&lt;/h4&gt;

&lt;p&gt;Let \(r\) be the smallest perturbations to the input \(x\) of a softmax
layer such that \(f(x+r) \neq f(x)\). The authors have shown that this
condition is then satisfied:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/softmax.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;h4 id=&#34;fully-connected-layers&#34;&gt;Fully connected layers&lt;/h4&gt;

&lt;p&gt;Here, the assumption is that the next layer has a robustness of \(k\). In that
case, the authors have shown that the minimum perturbations \(r\) satisfies
this condition:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/fulllayers.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Here, \(J(x)\) is the Jacobian matrix of \(h_L\) at \(x\), and \(M\) is the
bound of the second order derivative of \(h_L\).&lt;/p&gt;

&lt;h4 id=&#34;convolutional-layers&#34;&gt;Convolutional layers&lt;/h4&gt;

&lt;p&gt;For a convolutional layer with filter tensor \(W \in R^{k \times c \times q
\times q}\), and input tensor \(X\), the adversarial perturbation \(R\)
satisfies this condition:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/convolutional.png&#34; width=&#34;200&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;h4 id=&#34;pooling-layers&#34;&gt;Pooling layers&lt;/h4&gt;

&lt;p&gt;For a MAX or average pooling layer, the adversarial perturbation \(R\)
satisfies:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/maxpooling.png&#34; width=&#34;200&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;And for \(L_p\) pooling:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/lppooling.png&#34; width=&#34;200&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;The authors conducted their experiments using the MNIST and CIFAR-10 datasets on
the LeNet network. For generating the adversarial perturbations for testing the
theoretical bounds, they used the fast gradient sign method (FGSM). They used a
binary search to find the smallest perturbation parameter of FGSM that resulted
in misclassification of a sample.&lt;/p&gt;

&lt;p&gt;In their experiments, the authors did not find any adversarial sample that
violated the theoretical lower bounds. The experimental and theoretical
perturbation norms can be seen in the following two tables:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/fgsm.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Summary of norms of adversarial perturbations found by FGSM
&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class6/theoretical.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Summary of theoretical bound of adversarial perturbations
&lt;/p&gt;

&lt;p&gt;It can be seen that the mean theoretical perturbation is much lower than the
experimental ones for both MNIST and CIFAR-10 datasets. The authors suggest this
is due to FGSM not being the most efficient attacking technique.&lt;/p&gt;

&lt;p&gt;In conclusion, the authors suggest it is still unclear how tight these
theoretical bounds are. It will be interesting to verify how close they are to
the minimal distortion achieved by more efficient attacking techniques.
Moreover, they have only tried their method on feedforward networks. Applying it
to recurrent networks is a possible future direction. Finally, they propose the
adoption of a precise characterization of networks in terms of tradeoff between
robustness and accuracy.&lt;/p&gt;

&lt;p&gt;&amp;mdash; Team Bus:&lt;br /&gt;
Anant Kharkar,
Atallah Hezbor,
Bhuvanesh Murali,
Mainuddin Jonas,
Weilin Xu&lt;/p&gt;

&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.10578.pdf&#34;&gt;[1]&lt;/a&gt; T.-W Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, L. Daniel &amp;ldquo;Evaluating the robustness of neural networks: An extreme value theory approach&amp;rdquo; &lt;em&gt;ICLR 2018&lt;/em&gt;, January 2018.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;[2]&lt;/a&gt; N. Carlini, D. Wagner &amp;ldquo;Towards evaluating the robustness of neural networks&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1608.04644&lt;/em&gt;, March 2017.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;[3]&lt;/a&gt; C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus. &amp;ldquo;Intriguing properties of neural networks&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1312.6199&lt;/em&gt;, 2013.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.08475.pdf&#34;&gt;[4]&lt;/a&gt; M. Hein, M. Andriushchenko. &amp;ldquo;Formal guarantees on the robustness of a classifier against adversarial manipulation&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1705.08475&lt;/em&gt;, 2017.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;[5]&lt;/a&gt; I.J. Goodfellow, J. Shlens, C. Szegedy. &amp;ldquo;Explaining and harnessing adversarial examples&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1412.6572&lt;/em&gt;, March 2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.10207&#34;&gt;[6]&lt;/a&gt; Nicholas Carlini, Guy Katz, Clark Barrett, David L. Dill &amp;ldquo;Provably Minimally-Distorted Adversarial Examples&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1709.10207&lt;/em&gt;, February 2018.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.01135&#34;&gt;[7]&lt;/a&gt; Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer &amp;ldquo;Reluplex: An efficient SMT solver for verifying deep neural networks.&amp;rdquo; &lt;em&gt;International Conference on Computer Aided Verification&lt;/em&gt;. Springer, Cham, 2017.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class 5: Adversarial Machine Learning in Non-Image Domains</title>
      <link>https://secml.github.io/class5/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class5/</guid>
      <description>

&lt;h2 id=&#34;beyond-images&#34;&gt;Beyond Images&lt;/h2&gt;

&lt;p&gt;While the bulk of adversarial machine learning work has focused on image classification, ML is being used for a vartiety of tasks in the real world and attacks (and defenses) for different domains need to be tailored to the purpose of the ML process.
Among the most significant uses of ML are natural language processing and voice recognition. Within these fields, attacks look very different from those on images. With images, individual pixels can be changed large amounts without making the image unrecognizable, whereas in voice recognition, the audio needs to remain smooth, but many transformations can be done that are not noticable to humans.
These attacks are becoming more and more significant as always active listening devices like Amazon Alexa are becoming more common in the public sector. A broadcast television attack could be used to access personal accounts and devices of thousands of people simultaneously without many of those being attacked even noticing.&lt;/p&gt;

&lt;h2 id=&#34;adversarial-audio&#34;&gt;Adversarial Audio&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Nicholas Carlini and David Wagner. &lt;em&gt;Audio Adversarial Examples: Targeted Attacks on Speech-to-Text&lt;/em&gt;. University of California, Berkeley. 5 January 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1801.01944.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If classic science fiction authors, painting grandiose visions of artificial intelligence in the future, could read some of the tech headlines of today, they might be surprised by how their predictions panned out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class5/headlines.png&#34; alt=&#34;&#34; title=&#34;Google Home and Alexa in the news&#34; /&gt;
  &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(https://google.com)&#34;&gt;&lt;em&gt;Slides by Team Bus&lt;/em&gt;&lt;/a&gt;
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;These news articles recount recent incidents in which smart home voice-controlled devices have been activated by voices in television advertisements and shows, rather than from a physically-present human voice. While these untargeted, accidental (we assume) occurrences perhaps make for amusing news, they reflect a significant potential vulnerability in speech-to-text systems against targeted, adversarial attacks.&lt;/p&gt;

&lt;p&gt;In their 2018 paper, authors &lt;a href=&#34;https://arxiv.org/pdf/1801.01944.pdf&#34;&gt;Carlini and Wagner&lt;/a&gt; develop and demonstrate a white-box attack against automated speech recognition systems: “given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase of choice”. That is, a virtually unaltered sample, which might sound, at worst, noisy or mildly distorted to human ears, will be translated as an entirely different set of characters by the system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class5/waveform_illustration.png&#34; alt=&#34;&#34; title=&#34;Illustration of the attack: adding small perturbations causes an audio waveform to transcribe to any desired target phrase.&#34; /&gt;
   &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(https://arxiv.org/pdf/1801.01944.pdf)&#34;&gt;&lt;em&gt;Audio Adversarial Examples&lt;/em&gt;&lt;/a&gt; [1]
   &lt;/div&gt;&lt;/p&gt;

&lt;h4 id=&#34;audio-distortion&#34;&gt;Audio distortion&lt;/h4&gt;

&lt;p&gt;The goal of this attack is twofold: (a) to add the least possible amount of distortion to the original audio, and (b) to cause the distorted audio to map to an arbitrary sequence of transcribed characters. This thus becomes an optimization problem.&lt;/p&gt;

&lt;p&gt;To measure distortion, we use \(dB(v)\) to represent the loudness of a given sample \(v\) measured in decibels. First, let the change between the original and distorted samples be&lt;/p&gt;

&lt;p&gt;$$dB_x(\delta) := dB(\delta) - dB(x)$$&lt;/p&gt;

&lt;p&gt;where \(x\) is original and \(\delta\) distorted. Then, let&lt;/p&gt;

&lt;p&gt;$$C(v) = \mathop{\arg\,\max}\limits_p Pr[p | f(v)]$$&lt;/p&gt;

&lt;p&gt;be the most likely transcribed phrase given some sample \(v\).&lt;/p&gt;

&lt;p&gt;Our optimization problem becomes thus:&lt;/p&gt;

&lt;p&gt;$$\text{minimize } dB_x(\delta)$$
$$\text{such that } C(x+\delta)=t$$
$$\text{with } x+\delta \in [-M,M]$$&lt;/p&gt;

&lt;p&gt;where \(t\) is the target transcribed phrase and \(M\) is the maximum representable value (no distorted samples that are out of range &amp;ndash; in the authors&amp;rsquo; case, this happens to be \(2^{15}\) dB)).&lt;/p&gt;

&lt;p&gt;However, due to the nonlinearity of \(C(\cdot)\), gradient descent fails, so instead we minimize&lt;/p&gt;

&lt;p&gt;$$dB_x(\delta) + C(\cdot) \cdot \ell(x+\delta,t)$$
$$\text{with } \ell(x&amp;rsquo;,t) = \text{CTC-Loss}(x&amp;rsquo;,t)$$&lt;/p&gt;

&lt;p&gt;where the \(\text{CTC-Loss}\) (Connectionist Temporal Classification Loss) function is just the negative log-likelihood function.&lt;/p&gt;

&lt;p&gt;After solving this optimization problem, the authors constructed targeted examples on the first 100 test instances of Mozilla&amp;rsquo;s Common Voice dataset, attempting to distort 10 of these to achieve 10 target incorrect transcriptions &amp;ndash; with 100% success for source-target pairs, and a mean perturbation magnititude of -31 dB.&lt;/p&gt;

&lt;p&gt;Even after changing the loss function to&lt;/p&gt;

&lt;p&gt;$$\ell(x,pi) = \sum_i \max (f(x)^i_{\pi^i}-\max_{t&amp;rsquo;\neq \pi^i} f(x)^i_{t&amp;rsquo;},0) $$&lt;/p&gt;

&lt;p&gt;which avoids reducing loss and forces it to be more strongly classified, the mean distortion was only reduced from -31 dB to -38 dB, still a barely-noticeable to unnoticeable change.&lt;/p&gt;

&lt;h4 id=&#34;attack-sources-and-targets&#34;&gt;Attack Sources and Targets&lt;/h4&gt;

&lt;p&gt;Attack models worked consistently regardless of context, meaning that an attack using white noise or non-speech is as easy to craft as one using speech. Targeting silence turned out to be even easier than targeting speech.&lt;/p&gt;

&lt;h4 id=&#34;robustness&#34;&gt;Robustness&lt;/h4&gt;

&lt;p&gt;Random pointwise noise of less than -30dB is sufficient to corrupt an attack; although stronger attacks can be crafted using larger dB changes. The attacks did manage to be MP3 resistent, which speaks to the minimization of loss in MP3 compression as well as the fidelity of the attack model.&lt;/p&gt;

&lt;p&gt;These attacks do become easier with longer sources, as the pacing of the target speech can be varied more, giving a larger attack space in which to work. In an ideal case for an attacker, they would be working with a prerecorded broadcast, which gives them a large continuous space to attack.
That said, Over-The-Air attacks were not viable due to the white noise and variation in speakers and the rooms. Hidden voice commands did, however, show some promise for broadcast attacks.&lt;/p&gt;

&lt;p&gt;Transferability was shown to hold, as it is somewhat fundamental to ML systems. As of yet, defence techniques on the ML layers have not yet been developed, although random noise added post-attack is a viable defence.&lt;/p&gt;

&lt;h2 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Suranjana Samanta and Sameep Mehta. &lt;a href=&#34;https://arxiv.org/pdf/1707.02812.pdf&#34;&gt;Towards Crafting Text Adversarial Samples&lt;/a&gt;. 2017.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. Although most of the prior works have been focused on synthesizing adversarial samples in the image domain, NLP based text classifiers can also be the focal point of such exploit. This paper introduces a method of crafting adversarial text samples by modification of the original samples. To be precise, the authors propose to modify the original text samples by deleting or replacing the important  words in the text or by introducing new words in the text sample. While crafting adversarial samples, the paper focuses on generating meaningful sentences which can pass off as legitimate from language viewpoint.&lt;/p&gt;

&lt;h3 id=&#34;approach&#34;&gt;Approach&lt;/h3&gt;

&lt;p&gt;Initially, the authors calculate contribution of each word towards determining the class-label.  A word in the text is highly contributing if its removal from the text is going to change the class probability value to a large extent. In their proposed approach, modification of the sample text by considering each word at a time, in the order of the ranking based on the class-contribution factor. For the modification purpose, a candidate pool for each word in sample text is created considering synonyms and typos of each of the words as well as the genre or sub-category specific keywords. The assumption behind choosing sub-category or genre-specific keywords based on the fact that certain words may contribute to positive sentiment for a particular genre but can emphasis negative sentiment for other kind of genre. For example, we can consider the sentence, &amp;ldquo;The movie was hilarious&amp;rdquo;. This indicates a positive sentiment for a comedy movie. But the same sentence denotes a negative sentiment for a horror movie.  Thus, the word &amp;lsquo;hilarious&amp;rsquo; contributes to the sentiment of the review based on the genre of the movie. Finally, replacement, addition or removal of words are performed on a given text sample at each iteration, so that the modified sample flips its class label.&lt;/p&gt;

&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;The paper performs their experimental results on two datasets: IMDB movie review dataset for sentiment analysis and twitter dataset for gender classification. They compare the efficiency of their method with the existing method TextFool by measuring the accuracy of the model
obtained at different configurations. For both the cases, the proposed model was generated using Convolutional Neural Network (CNN).&lt;/p&gt;

&lt;p&gt;The figure below (taken from the paper) shows the results for IMDB dataset. From the figure, it is obvious that the proposed method of adversarial sample crafting for text is capable of synthesizing semantically correct adversarial text samples from the original text sample. In addition, the inclusion of genre specific keywords appears to increase the quality of sample crafting. This is evident from the fact the drop in accuracy of the classifier before re-training for original text sample and the adversarialy crafted text sample is more when genre specific keywords are being used.
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/result_imdb.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt;  &lt;b&gt;Figure:&lt;/b&gt; Performance results on IMDB movie review dataset.
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;A significant factor for evaluating the effectiveness of adversarial samples is to measure the semantic similarity between the original samples and their corresponding tainted counterparts. A lower similarity score denotes that the semantic meaning of the original and the modified samples are quite different, which is not acceptable from the language viewpoint. The authors found the average semantic similarity between the original text sample and their adversarial counterparts (for test set only) as 0.9164 and 0.9732 with and without using the genre specific keywords respectively. Although the semantic similarity between the original and their corresponding perturbed samples does decrease a bit while the genre specific keywords in the candidate pool, the number of valid adversarial samples generated increases as it can be seen from the percentage of the perturbed samples for genre specific keywords in the above figure.&lt;/p&gt;

&lt;p&gt;Another key component for the evaluation of adversarial samples is to measure the number of changes incurred to obtain the adversarial sample. The number of changes made to craft a successful adversarial sample should be ideally low.The following figure shows a graph that indicates the number of changes required to create successful adversarial samples with and without using the genre-specific keywords. From the figure, it is evident that, if we consider same number of modifications the number of tainted sample produced using genre specific keywords for creating adversarial samples is more than that when genre specific keywords are not used.
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/result_modifications.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt;  &lt;b&gt;Figure:&lt;/b&gt; Plot showing the number of adversarial samples produced against the number of changes incurred
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;For the Twitter dataset, the proposed method shows similar performance as the IMDB dataset.
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/result_twitter.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt;  &lt;b&gt;Figure:&lt;/b&gt; Performance results on Twitter gender classification dataset.
&lt;/p&gt;&lt;/p&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/example_nlp.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt;  &lt;b&gt;Figure:&lt;/b&gt; Examples of adversarial samples crafted from Twitter and IMDB dataset using (i)TextFool and
(ii) proposed method.
&lt;/p&gt;

&lt;h4 id=&#34;face-recognition&#34;&gt;Face Recognition&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter. &lt;a href=&#34;https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf&#34;&gt;&lt;em&gt;Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition&lt;/em&gt;&lt;/a&gt;. ACM CCS 2016.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Face recognition and face detection algorithms are extensively used in surveillance and access control applications. This paper focuses on fooling the state-of-art machine learning algorithms that are practically deployed for these tasks. More concretely, the paper carries out two types of attacks: dodging and impersonation. In dodging, the attacker tries to conceal his/her identity, whereas, in impersonation, the attacker tries to trick the algorithm into recognizing him/her as a different target individual. The authors realize these attacks by printing a wearable glass which, upon wearing, allows the attacker to successfully launch the attacks. The glasses used in the attack are shown below.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/glasses.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Glass frames used for the attacks
&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf&#34;&gt;Parikh et al.&lt;/a&gt; proposed the state-of-art 39 layer deep neural network trained on 2,622 celebrities which achieved an accuracy of 98.95% for the task of face detection. The authors of this paper use this model to launch their attacks.&lt;/p&gt;

&lt;p&gt;The objective function of the deep neural network&amp;rsquo;s softmax layer is given as below:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/objective.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;For impersonation, the objective is to add the minimum amount of noise \(r\) in the input image \(x\) to convert the class lable to the target label \(c_t\), as given below:
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/impersonation_obj.png&#34; width=&#34;450&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;For dodging, the objective is to add minimum amount of noise \(r\) in the input image \(x\) to deviate from the correct class label \(c_x\).
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/dodging_obj.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;The noises are then carefully added to the 3D printed glasses.&lt;/p&gt;

&lt;h4 id=&#34;experiments-1&#34;&gt;Experiments&lt;/h4&gt;

&lt;p&gt;The paper launches successful dodging and impersonation attacks. Figure below shows an example of impersonation where a female celebrity (left) is classified as a male celebrity (right) by the algorithm when the glass frame is used (center).
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/example1.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Successful impersonation using the glasses
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Figure below shows an example of dodging the face detection algorithm with minor changes to the image that donot affect a normal human judgement. Left image is the original image and the center and right images are the perturbed images where the algorithm does not detect the faces.
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class5/example2.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Dodging the face detection
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Further results show 100% dodging success rate and very high impersonation success rate.&lt;/p&gt;

&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;

&lt;p&gt;While the authors perform white-box attacks on the state-of-the-art face detection and face recognition algorithms, they also discuss about successfully carrying out the black-box attacks.&lt;/p&gt;

&lt;p&gt;— Team Nematode: &lt;br /&gt;
Bargav Jayaraman, Guy &amp;ldquo;Jack&amp;rdquo; Verrier, Joshua Holtzman, Max Naylor, Nan Yang, Tanmoy Sen&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.01944.pdf&#34;&gt;[1]&lt;/a&gt; Nicholas Carlini and David Wagner, &amp;ldquo;Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.&amp;rdquo; January 2018.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf&#34;&gt;[2]&lt;/a&gt; Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter, &amp;ldquo;Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition.&amp;rdquo; October 2016.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class 4: Differential Privacy In Action</title>
      <link>https://secml.github.io/class4/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class4/</guid>
      <description>

&lt;p&gt;Two weeks ago we took a look at &lt;a href=&#34;https://secml.github.io/class2/&#34;&gt;privacy in machine learning&lt;/a&gt; and introduced differential privacy as one possible approach to perform statistical analysis on data while maintaining user privacy.  Today we explore three applications of differential privacy: Google&amp;rsquo;s RAPPOR for obtaining user data from client-side software, the FLEX system to enforce differential privacy for SQL queries, and an algorithm for training deep neural networks that can provide differential privacy guarantees.&lt;/p&gt;

&lt;h2 id=&#34;google-s-rappor&#34;&gt;Google&amp;rsquo;s RAPPOR&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Erlingsson, Úlfar, Vasyl Pihur, and Aleksandra Korolova. &lt;em&gt;Rappor: Randomized aggregatable privacy-preserving ordinal response&lt;/em&gt;. ACM CCS 2014. [&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal of Randomized Aggregatable Privacy-Preserving Ordinal Response (RAPPOR) is to ensure anonymity for those participating in crowd-sourced statistics with a strong privacy guarantee. For example, if student who has cheated is participating in a study on cheating in school it is unlikely they would respond truthfully without strong plausible deniability.&lt;/p&gt;

&lt;p&gt;A simple algorithm that constructs such plausible deniability is shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;STEP ONE

flip coin
if (coin==HEADS):
    answer truthfully
else:
    go to step 2

STEP TWO

flip coin
if (coin==HEADS):
    answer yes
else:
    answer truthfully
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This algorithm will result in a truthful response rate of 75%. Therefore if we let \(Y = [\text{Number of raised hands}] / [\text{Size of class}]\) then \([\text{Number of true cheaters}] \approx 2(Y − 0.25)\).&lt;/p&gt;

&lt;p&gt;This means we have differential privacy with the level \(\epsilon = \ln(0.75/(1 − 0.75)) = \ln(3)\), but only for the first time responses are collected. This guarantee degrades if the same survey is administered repeatedly (where the same respondants generate new randomness for each query). RAPPOR is a way to ensure strong privacy protection even for a single respondent who is surveyed often.&lt;/p&gt;

&lt;p&gt;On a high level, RAPPOR achieves this by having each client machine report a “noisy” representation of the true value \(v\) by submitting a \(k\)-sized bit array to a server. This representation of \(v\) is selected in order to reveal a specific amount of information about \(v\) in order to limit the information the server learns from the \(k\)-sized bit array. Importantly the server does not learn the true value \(v\) with confidence even when an infinite number of reports are submitted by the client.&lt;/p&gt;

&lt;p&gt;This is achieved through the the following three steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Signal&lt;/em&gt; - hash the client’s value \(v\) onto the Bloom filter \(B\) of size \(k\) using \(h\) hash functions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/rappor_signal.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Bloom Filter
&lt;/p&gt;

&lt;p&gt;A Bloom filter is a probabilistic data structure optimized for determining set membership.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Permanent&lt;/em&gt; Randomized Response
For each value \(v\) from a client, and for each bit \(i\) in the bloom filter \(B\), create a binary reporting value \(B_i’\) such that TODO:missing equation here??? where \(f\) is a user-defined tuning variable, \(f \in (0,1)\). This \(B’\) is memorized and reused as the basis for all future reports on value \(v\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/rappor_perm_prob.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Permanent B&#39;
&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Instantaneous Randomized Response&lt;/em&gt;. Next, allocate a bit array \(S\) of size \(k\) and initialize to 0. Set each bit \(i\) in \(S\) with the following probabilities: TODO:???&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/rappor_inst_prob.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Report&lt;/em&gt;. Send the bit array \(S\) to the server.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How private are RAPPOR’s aggregated results in reality? If you assume infinite sampling, there is a privacy guarantee of:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/rappor_epsilon_inf.png&#34; width=&#34;500&#34; &gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;An attacker with &amp;ldquo;unlimited collection capabilities&amp;hellip; is also bounded by the privacy guarantee of ε∞ and cannot improve upon this bound with more data collection.&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;towards-practical-differential-privacy-for-sql-queries&#34;&gt;Towards Practical Differential Privacy for SQL Queries&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Johnson, Noah, Joseph P. Near, and Dawn Song. &lt;em&gt;Towards Practical Differential Privacy for SQL Queries&lt;/em&gt;. Proceedings of the VLDB Endowment, January 2018. &lt;a href=&#34;Images below are taken from this paper.&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.09479.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;motivation&#34;&gt;Motivation&lt;/h4&gt;

&lt;p&gt;The recent increase in data collection in large organizations has exposed personal user data to analysis that have an impact on user privacy. This paper investigates the methods that can be used to increase differential privacy, protecting individual privacy. By taking a practical approach to differential privacy, the authors are able to provide an implementation of differential privacy that is compatible with any existing database.&lt;/p&gt;

&lt;h4 id=&#34;contributions&#34;&gt;Contributions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Empirical study of 8.1 million real-world SQL queries&lt;/li&gt;
&lt;li&gt;Elastic sensitivity&lt;/li&gt;
&lt;li&gt;FLEX: An end-to-end differential privacy system&lt;/li&gt;
&lt;li&gt;Experimental evaluation of FLEX&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;elastic-sensitivity&#34;&gt;Elastic sensitivity&lt;/h4&gt;

&lt;p&gt;Elastic sensitivity is the paper’s main contribution and proposed approach to enforcing differential privacy on a system that is locally sensitivity-based. Local sensitivity is the “maximum of the difference between the query run on a true database and any neighbor of it.” The practical nature of their design exists because instead of being a property of all possible databases, local sensitivity is a property of one true database.
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/ls.png&#34; width=&#34;300&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Local Sensitivity
&lt;/p&gt;
The paper describes and proves their theorem shown below using aspects of database local sensitivity.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/proof.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Elastic Sensitivity
&lt;/p&gt;

&lt;h4 id=&#34;limitation-of-existing-approaches&#34;&gt;Limitation of existing approaches&lt;/h4&gt;

&lt;p&gt;Two main requirements of differential privacy are stated. The first is that the implementation of differential privacy must be compatible with existing databases, and have support for heterogeneous databases. Also it should have robust support for equijoins, which include self and non-self joins, relationship types and an arbitrary number of nested joins.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/g1.PNG&#34; width=&#34;600&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Comparison of various differential privacy implementations
&lt;/p&gt;

&lt;p&gt;As seen in the chart above, other approaches address parts of the requirements, and elastic sensitivity seems to cover all of the requirements set forth in this paper for differential privacy. One lacking requirement among previous approaches was adequate database compatibility.&lt;/p&gt;

&lt;h4 id=&#34;unsupported-queries-and-other-aggregate-functions&#34;&gt;Unsupported Queries and other aggregate functions&lt;/h4&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/sqlcode.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; SQL non-equijoin statement
&lt;/p&gt;

&lt;p&gt;Since non-equijoins need information about both datasets, the operation is not supported in elastic sensitivity. Elastic sensitivity also sometimes fails to provide support for max-frequency metrics. However, not having support for either of these operations does not impact a majority of operations in either case.&lt;/p&gt;

&lt;h4 id=&#34;flex&#34;&gt;FLEX&lt;/h4&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/flexPerf.PNG&#34; width=&#34;500&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; FLEX design structure
&lt;/p&gt;

&lt;p&gt;FLEX is an implementation of elastic sensitivity that is highly compatible with existing databases. The FLEX database structure is shown in the figure above. TODO: this is the wrong figure&lt;/p&gt;

&lt;p&gt;By targeting support for specific SQL queries, their system can enforce differential privacy on most real world queries to databases. For a given SQL query, FLEX calculates its elastic sensitivity given an analysis of the query. FLEX then applies smooth sensitivity to the elastic sensitivity and adds noise drawn from the Laplace distribution to the original query results.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/flexPerf.PNG&#34; width=&#34;600&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; FLEX Performance
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Elastic sensitivity for 76% of queries&lt;/li&gt;
&lt;li&gt;Large error for unsupported queries: 14.14%&lt;/li&gt;
&lt;li&gt;Parsing errors: 6.58%&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;comparison-with-wpinq&#34;&gt;Comparison with wPINQ&lt;/h4&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/wPINQComp.PNG&#34; width=&#34;1000&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; wPINQ and Flex performance comparison
&lt;/p&gt;

&lt;p&gt;This paper moves towards more practical approaches of differential privacy in SQL queries by proposing elastic sensitivity. The concept of elastic sensitivity was then tested through their implementation in FLEX. A comparison of FLEX and wPINQ is shown in the figure above. Adoption of this method by Uber for internal data analytics demonstrates the potential of their approach for having a large impact on data privacy.&lt;/p&gt;

&lt;h2 id=&#34;deep-learning-with-differential-privacy&#34;&gt;Deep learning with differential privacy&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kinal Talwar, and Li Zhang. &lt;em&gt;Deep learning with differential privacy&lt;/em&gt;. ACM CCS 2016 &lt;a href=&#34;All figures below are taken from this paper&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.00133.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;differential-privacy-recap&#34;&gt;Differential Privacy Recap&lt;/h4&gt;

&lt;p&gt;Differential Privacy can be defined in terms of the application-specific concept of adjacent databases. Suppose, for adjacent databases where each training dataset contains a set of image-label pairs, we say that two of these sets are adjacent if one image-label pair is present in one training set while absent in the other. A randomized mechanism \( \mathcal{M}: D \rightarrow R\) with domain D and range R satisfies \( (\epsilon, \delta) \)-differential privacy if for any two adjacent inputs \( d,d&amp;rsquo;~\epsilon ~D\) and for any subset of outputs \( S \subseteq R\) it holds that&lt;/p&gt;

&lt;p&gt;$$ Pr[\mathcal{M}(d) ~\epsilon ~\mathcal{S}] \leq \mathcal{e}^{\epsilon} Pr[\mathcal{M}(d&amp;rsquo;) ~\epsilon ~\mathcal{S}] + \delta $$&lt;/p&gt;

&lt;p&gt;Authors used Dwork et al.[1] privacy definition of allowing the possibility that plain \( \epsilon \)-differential privacy is broken with probability \( \delta \) in their work.&lt;/p&gt;

&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;

&lt;p&gt;Their proposed technique consists of three components: differentially private SGD (Stochastic Gradient Descent) Algorithm, Moments Accountant and Hyperparameter Tuning.&lt;/p&gt;

&lt;h4 id=&#34;differentially-private-sgd-algorithm&#34;&gt;Differentially private SGD Algorithm&lt;/h4&gt;

&lt;p&gt;Algorithm 1 describes their method for training a model with parameters \( \theta \) by minimizing the loss function \( L(\theta) \). At each step of computing of SGD, they calculated the gradient \( \nabla \theta L(\theta, x_i)\) for a random subset then clip the \(L_2\) norm of each gradient. After that they computed the average and for ensuring the privacy they added noise in those. They took the opposite direction of this average noisy gradient. Finally, they ouput the model with the privacy loss.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/algorithm_diff_priv_sgd.png&#34; width=&#34;600&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Code snippet of Differentially private SGD
&lt;/p&gt;

&lt;h4 id=&#34;moments-accountant&#34;&gt;Moments Accountant&lt;/h4&gt;

&lt;p&gt;Each lot is \( (\epsilon, \delta)\)-DP if we choose \(\sigma \) in Algorithm 1 as \( \sigma \) = \(\frac{\sqrt{ 2\log(\frac{1.5}{\delta})}} {\epsilon} \) for Gaussian noise. Thus, each step is \( \mathcal{O}((q \epsilon),q\delta) \)-DP over the dataset, where q = \( \frac{L}{N}\) is the sampling probability of a lot over the dataset and \(\epsilon \leq 1 \).
The result in the literature which gives the best overall bound is the strong composition theorem [2]. Strong composition theorem does not take into account any particular noise distribution under consideration. Authors invent a stronger accounting method, which is the moments accountant. In Algorithm 1, over \(T\) iterations, naive composition gives the bound of \( \mathcal{O}((qT \epsilon),qT\delta) \)-DP. Over \(T\) iterations, strong composition gives the bound of \( \mathcal{O}((q \epsilon \sqrt{T \log \frac{1}{\delta}}),qT\delta) \)-DP. Whereas, over \(T\) iterations, moments accountant gives a tighter bound of \( \mathcal{O}((q \epsilon \sqrt{T}),\delta) \)-DP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; There exist constants \(c_1\) and \(c_2\) so that given the sampling probability q = L/N and the number of steps T, for any \( \epsilon &amp;lt; c_1q^{2}T \), Algorithm 1 is \( (\epsilon,\delta) \)-differentially private for any \(\delta &amp;gt; 0\) if we choose $$ \sigma \geq c_2 \frac{q \sqrt{T\log(\frac{1}{\delta})}}{\epsilon} $$&lt;/p&gt;

&lt;p&gt;If we use the strong composition theorem, we will then need to choose \( \sigma = \Omega(q\sqrt{T\log(1/\delta)\log(T/\delta)}/\epsilon)\)
For \(L = 0.01N\), \(\sigma\) = 4, \(\delta\) = \(10^{-5}\), and \(T = 10000\), we have \(\epsilon\) ≈ 1.26 using the moments accountant, and \(\epsilon\) ≈ 9.34 using the strong composition theorem.&lt;/p&gt;

&lt;h4 id=&#34;hyperparameter-tuning&#34;&gt;Hyperparameter Tuning&lt;/h4&gt;

&lt;p&gt;Authors used the insight from the version of a result from Gupta et al. [3] restated as Theorem D.1 in the Appendix to reduce the number of hyperparameter settings. From theorem 1, one can say that making batches size too large will increase the privacy cost. The learning rate in non-private training will also go downwards as the model will converge to a local optimum. But authors found that they didn&amp;rsquo;t have to decrease the learning rate to a very small value because differentially private training never goes to an area where it would be justified. Moreover, from their experiment they came to know that there was a small advantage of starting with a large learning rate compare to small learning rate and then linearly decaying it to a smaller value in a few epochs. And after that the rate remained constant.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;They have implemented the differentially private SGD algorithms in TensorFlow. They made their code available to download under an Apache 2.0 license from github.com/tensorflow/models. Their whole implementation is divided into two components. One is Sanitizer which preprocesses the gradient to protect privacy and the other one is privacy accountant which keeps track of the privacy cost of training the model. They implemented differentially private PCA(Principal Directions) and applied pre-trained convolutional layers. Because the neural network model may benefit by projecting the inputs on the PCA or by feeding it through a convolutional layer.&lt;/p&gt;

&lt;h4 id=&#34;sanitizer&#34;&gt;Sanitizer&lt;/h4&gt;

&lt;p&gt;For achieving the privacy protection, the sanitizer performs the following two operations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Limits sensitivity of each example by clipping the norm of the gradient&lt;/li&gt;
&lt;li&gt;Adds noise to the gradient of a batch before updating network parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;privacy-accountant&#34;&gt;Privacy Accountant&lt;/h4&gt;

&lt;p&gt;The main purpose of their implementation is to keep track of privacy spending over the course of training and hence the implementation of Privacy Accountant is the most important component of their implementation. One can compute \(\alpha (\gamma)\) by two ways. First one is by applying asymptotic bound and then evaluating a closed-form expression and the second one is by applying numerical integration. In their implementation they used the second one to compute \(\alpha (\gamma)\). They also \(\alpha (\gamma)\) computed for a wide range of \( \gamma&amp;rsquo;s\), which helped them to get the best result possible \( (\epsilon, \delta) \) values. They stated that for the parameters it is enough to calculate \(\alpha (\gamma)\) where \(\gamma \leq 32\).&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/algorithm_dpsgd_optimizer.png&#34; width=&#34;600&#34; &gt;
&lt;br&gt; &lt;b&gt;Figure:&lt;/b&gt; Code snippet of DPSGD_Optimizer and DPTrain
&lt;/p&gt; 

&lt;p&gt;The above figure contains the TensorFlow code snippet (in Python) of DPSGD_Optimizer which iteratively invokes DPSGD_Optimizer using a privacy accountant to bound the total privacy loss.&lt;/p&gt;

&lt;h4 id=&#34;differentially-private-pca&#34;&gt;Differentially private PCA&lt;/h4&gt;

&lt;p&gt;Differentially Private Principal Component Analysis is a very useful method for capturing the features of the input data. Authors implemented the differentially private PCA algorithm according to Dwork et al.[4]. They took a random sample from the training examples and treated them as vectors. They normalized each vector to unit \(l_2\) norm to form the matrix \(A^{T}A\). After that they added Gaussian noise to the matrix A. Then for each input example they applied the projection to the principal directions of noisy covariance matrix before feeding them into the neural network.&lt;/p&gt;

&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;

&lt;h4 id=&#34;mnist&#34;&gt;MNIST&lt;/h4&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/result_accuracy_mnist_fig_3.png&#34;&gt;
&lt;/p&gt; 

&lt;p&gt;&lt;strong&gt;Figure:&lt;/strong&gt; Results on the accuracy for different noise levels on the MNIST dataset. In all the experiments, the network uses 60 dimension PCA projection, 1,000 hidden units, and is trained using lot size 600 and clipping threshold 4. The noise levels \(\sigma, \sigma_{p}\) for training the neural network and for PCA projection are set at (8, 16), (4, 7), and (2, 4), respectively, for the three experiments.&lt;/p&gt;

&lt;p&gt;In the above Figure, they showed the results for different noise levels. In
each plot, they showed the evolution of the training and testing
accuracy as a function of the number of epochs as well as
the corresponding \(\delta\) value, keeping \(\epsilon\) fixed.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/result_accuracy_mnist_fig_4.png&#34; width=&#34;600&#34; &gt;
&lt;/p&gt; 

&lt;p&gt;&lt;strong&gt;Figure:&lt;/strong&gt; Accuracy of various \( (\epsilon,\delta) \) privacy values
on the MNIST dataset. Each curve corresponds to a different \(\delta \) value.&lt;/p&gt;

&lt;p&gt;In the above figure, they showed the accuracy that they can obtain for different value of \(\delta\) and \(\epsilon\). From the figure, one can observe that keeping the value of \(\delta\) fixed and varying the value of \(\epsilon\) can have large impact on the accuracy.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/result_accuracy_mnist_fig_5.png&#34; &gt;
&lt;/p&gt; 

&lt;p&gt;&lt;strong&gt;Figure:&lt;/strong&gt; MNIST accuracy when one parameter varies, and the others are fixed at reference values.&lt;/p&gt;

&lt;p&gt;In the above figure, they showed the accuracy of the model on MNIST dataset by by varying a particular parameter while keeping the other parameter constant. For training the neural network parameters and for PCA projection they set the reference values like - 60 PCA dimenstions, 600 lot size, 1000 hidden units, initial learning rate of 0.1, final learning rate 0.052 in 10 epochs, gradient norm bound of 4, and noise equal to 4 and 7 respectively.&lt;/p&gt;

&lt;h4 id=&#34;cifar-10&#34;&gt;CIFAR-10&lt;/h4&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/class4/result_accuracy_cifar_fig_6.png&#34; &gt;
&lt;/p&gt; 

&lt;p&gt;&lt;strong&gt;Figure:&lt;/strong&gt; Results on accuracy for different noise levels on CIFAR-10. With \(\delta \) set to \(10^{-5}\) , achieve accuracy 67%, 70%, and 73%, with \(\epsilon \) being 2, 4, and 8, respectively. The first graph uses a lot size of 2,000, (2) and (3) use a lot size of 4,000. In all cases, \(\sigma \) is set to 6, and clipping is set to 3.&lt;/p&gt;

&lt;p&gt;In the above figure, authors showed the result of the accuracy and privacy cost of the model on the CIFAR-10 dataset as a function of the number of epochs for different parameter settings. In MNIST dataset, the difference in accuracy between a non-private baseline and a private model is about 1.3% whereas the corresponding drop in accuracy in CIFAR-10 dataset is about 7%.&lt;/p&gt;

&lt;p&gt;&amp;mdash; Team Panda
Christopher Geier, Faysal Hossain Shezan, Helen Simecek, Lawrence Hook, Nishant Jha&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.wisdom.weizmann.ac.il/~naor/PAPERS/odo.pdf&#34;&gt;[1]&lt;/a&gt; C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,and M. Naor. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT, pages 486–503. Springer, 2006.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://privacytools.seas.harvard.edu/files/privacytools/files/05670947.pdf&#34;&gt;[2]&lt;/a&gt; C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS, pages 51–60. IEEE, 2010.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/0903.4510.pdf&#34;&gt;[3]&lt;/a&gt; A. Gupta, K. Ligett, F. McSherry, A. Roth, and K. Talwar. Differentially private combinatorial optimization. In SODA, pages 1106–1125, 2010.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kunaltalwar.org/papers/PrivatePCA.pdf&#34;&gt;[4]&lt;/a&gt; C. Dwork, K. Talwar, A. Thakurta, and L. Zhang. Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis. In STOC, pages 11–20. ACM, 2014.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Topic Suggestions</title>
      <link>https://secml.github.io/topic-suggestions/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/topic-suggestions/</guid>
      <description>&lt;p&gt;To help with topics for future classes, I&amp;rsquo;ve created a
&lt;a href=&#34;https://secml.github.io/topics&#34;&gt;Topics&lt;/a&gt; page with some possible ideas and links to papers.
This is not meant to be exhaustive by any stretch; any topics loosely
connected to machine learning security and privacy is within scope,
and there are lots of great papers on these topics not included on
this page.  But, hopefully it will be a useful starting point for
teams looking for ideas for topics for future classes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class 3: Adversarial Machine Learning</title>
      <link>https://secml.github.io/class3/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class3/</guid>
      <description>

&lt;p&gt;This week’s topic covered some proposed adversarial example attacks and defenses. The underlying problem is that machine learning techniques assume that training and testing data are generated from the same distribution. Therefore, adversaries can choose inputs to exploit the algorithms by manipulating data. We began class by discussing common distance metrics,
\(L_0, L_2\), and \(L\infty\), popular benchmarking datasets, and the history of adversarial ML. However, the main theme was defense techniques can be used safely to prevent adversarial attacks. Below we discuss four papers that discuss both effective and ineffective defenses.&lt;/p&gt;

&lt;h2 id=&#34;distillation-as-a-defense&#34;&gt;Distillation as a Defense&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Nicolas Papernot and
               Patrick D. McDaniel,
               Xi Wu,
               Somesh Jha, and
               Ananthram Swami. &lt;em&gt;Distillation as a Defense to Adversarial Perturbations against Deep
               Neural Networks&lt;/em&gt;. IEEE Symposium on Security and Privacy, 2016. [&lt;a href=&#34;https://arxiv.org/abs/1511.04508&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;what-is-distillation&#34;&gt;What is Distillation&lt;/h3&gt;

&lt;p&gt;Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, \(z_i\), computed for each class into a probability, \(q_i\), by comparing \(z_i\) with the other logits.&lt;/p&gt;

&lt;p&gt;$$ q_i = \frac{\exp(z_i/T)}{\sum_j exp(z_j/T)} $$&lt;/p&gt;

&lt;p&gt;where \(T\) is a temperature that is normally set to 1. Using a higher value for \(T\) produces a softer probability distribution over classes.&lt;/p&gt;

&lt;p&gt;In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1. It could reduce the computing resources required to run a network, allowing usage on a smaller scale like in embedded chips and IoT devices.&lt;/p&gt;

&lt;h3 id=&#34;how-does-distillation-work&#34;&gt;How does Distillation Work?&lt;/h3&gt;

&lt;p&gt;1) A Deep Neural Network(DNN) is trained with a high temperature, the \(T\) we mentioned before. The training of this first DNN is a high temperature because the high temperature forces the DNN to produce probability vectors with relatively large values for each class. The high temperature of a softmax is, the more ambiguous its probability distribution will be. The smaller the temperature of a softmax is, the more discrete its probability distribution will be.&lt;/p&gt;

&lt;p&gt;2) A second Deep Neural Network is trained by replacing the hard labels of the training set with class probabilities output by the first Deep Neural Network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/DNN.png&#34; alt=&#34;&#34; /&gt;
&lt;div class=&#34;caption&#34;&gt;TODO: add source&lt;/div&gt;&lt;/p&gt;

&lt;h3 id=&#34;softmax-function-under-distillation&#34;&gt;Softmax Function under distillation&lt;/h3&gt;

&lt;p&gt;Softmax function is the Last layer of network. It’s used to normalize the outputs of the second to last layer. Under distillation situation, it has a parameter temperature (\(T\)).
To perform distillation in softmax layer, a large network whose output layer is softmax is first trained on the original dataset. The softmax layer is a layer that considers a vector \(Z(X)\) of outputs produced by the last hidden layer of a DNN. Then we normalizes them into a probability vector \(F(X)\), the output of DNN assigning a probability to each class of dataset for input \(X\). \(T\) means temperature and shared across the softmax layer.  (See the paper for the equations for \(F(X)\).)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/softmax.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;using-distillation-as-a-defense&#34;&gt;Using Distillation as a Defense&lt;/h3&gt;

&lt;p&gt;In distillation as a defense, the same network architecture is used in the distilled DNN as in the original DNN. First, this paper trained an initial network \(F\) on data \(X\) with a softmax temperature of \(T\).&lt;/p&gt;

&lt;p&gt;Then, this paper uses the probability vector \(F(X)\), which includes additional knowledge about classes compared to a class label, predicted by network \(F\) to train a distilled network  at temperature \(T\) on the same data \(X\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/defense.png&#34; alt=&#34;&#34; /&gt;
&lt;div class=&#34;caption&#34;&gt;TODO: add source&lt;/div&gt;&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;This paper evaluated Resilience, Sensitivity and Robustness on 2 datasets: MNIST and CIFAR10&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/table.png&#34; alt=&#34;&#34; /&gt;
&lt;div class=&#34;caption&#34;&gt;
Resilience: success rate of adversarial crafting.
&lt;/div&gt;
&lt;div class=&#34;caption&#34;&gt;
  Sensitivity: amplitude of adversarial gradients.
&lt;/div&gt;
&lt;div class=&#34;caption&#34;&gt;
  Robustness: amount of perturbation required to achieve adversarial targets.
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This paper also evaluated effect of Temperature on Adversarial Success
Success of adversarial samples when changing at most 112 features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/res.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These results seem promising, but subsequent work found that Defensive
Distillation is not actually robust to adversarial examples.&lt;/p&gt;

&lt;h3 id=&#34;why-distillation-seems-to-work&#34;&gt;Why Distillation Seems to Work&lt;/h3&gt;

&lt;p&gt;First some attacks use the gradient of the logits, where softmax do not work here. Second, big difference in relative impact of changes in input to the softmax layer. Third, training at temperature \(T\) effectively increases all inputs to the softmax layer by a factor of \(T\), for example, Undistilled logits with Mean of 5.8/std of 6.4 and Distilled logits (T=100) with Mean of 482/std of 457.&lt;/p&gt;

&lt;h3 id=&#34;breaking-distillation&#34;&gt;Breaking Distillation&lt;/h3&gt;

&lt;p&gt;Instead of using the gradient of the input to the softmax layer, the gradient of the output of the softmax layer was used.Due to the problem of vanishing gradients, artificially divide the inputs to the softmax by \(T\). This method achieves a successful misclassification rate of 96.4%.&lt;/p&gt;

&lt;h2 id=&#34;towards-evaluating-the-robustness-of-neural-networks&#34;&gt;Towards Evaluating the Robustness of Neural Networks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Carlini, Nicholas, and David Wagner. &lt;em&gt;Towards evaluating the robustness of neural networks&lt;/em&gt;. IEEE Symposium on Security and Privacy (&amp;ldquo;Oakland&amp;rdquo;) 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;Carlini et al.&lt;/a&gt; proposed an optimization based attack to break the distillation defense mechanism developed by Papernot et al and other undistilled networks. This poses severe security problems to machine learning classifiers as the current defense strategy becomes vulnerable to this new type of strong white-box attacks. The attack is based on optimization approach, where the problem is formulated as&lt;/p&gt;

&lt;p&gt;$$ \text{minimize}~\mathcal{D}(x,x+\delta) + cf(x+\delta)\\ \text{such that:}~x+ \delta \in [0,1]^{n} $$&lt;/p&gt;

&lt;p&gt;where \(f(x+\delta) \leq 0\) iff the classification result of \((x+\delta)\) is in class \(t\) (i.e. \(C(x+\delta) = t)\). \(c\) is a coefficient controling the relative importance of misclassification and norm minimization. Intuitive understanding of the optimization problem above is, we want a smaller perturbation (unobservable by human) by minimizing the norm \(||\delta||\) and causing misclassification by minimizing the function \(f(x+\delta)\). The constraint of \(x+\delta \in [0,1]^{n}\) is because this attack is conducted in image domain and has to generate valid images whose pixel value is in \([0,1]\) range. In order to further avoid the constraint, the authors apply \(tanh\) function to change the variables. Specifically, \(\delta_i = \frac{1}{2}(tanh(w_i)+1)-x_i \) and we directly optimize the unconstrained variable \(w_i \) instead of \(\delta_i\). &lt;a href=&#34;https://arxiv.org/pdf/1412.6980.pdf&#34;&gt;ADAM&lt;/a&gt; is deployed to solve the above unconstrained optimization problem. A special note is the author implemented different \(p\)-norms \((p = 0, 2, \infty)\) under the optimization framework described above.&lt;/p&gt;

&lt;p&gt;The evaluation part is compared to other existing white-box attack methods. Specifically, &lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;Fast Gradient Sign (FGS)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1607.02533.pdf&#34;&gt;Iterative Fast Gradient Sign (IFGS)&lt;/a&gt; methods are suitable for \(L_\infty \)-norm, &lt;a href=&#34;https://arxiv.org/pdf/1511.04599.pdf&#34;&gt;Deepfool&lt;/a&gt; method is suitable for \(L_2\)-norm attack, &lt;a href=&#34;https://arxiv.org/pdf/1511.07528.pdf&#34;&gt;JSMA&lt;/a&gt; method is suitable for \(L_0\)-norm attack. The target model for the listed attacks are &lt;a href=&#34;https://arxiv.org/pdf/1511.04508.pdf&#34;&gt;distillation based network&lt;/a&gt; (defensitive strategy proposed for defending against FGS and IFGS methods) and normally trained neural networks. The final evaluation results demonstrate that the attack proposed in this work is far stronger than existing attack method in that it achieves highest attack success rate (this method achieves \(100\%\) success rate while other methods don&amp;rsquo;t) and lowest perturbation magnitude. Some sample images generated from this attack are shown below. This attack is also now broadly accepted as a standard baseline for evaluating newly proposed defense mechanisms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/class3_img1.png&#34; alt=&#34;&#34; /&gt;
&lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(https://arxiv.org/pdf/1608.04644.pdf)&#34;&gt;&lt;em&gt;Towards evaluating the robustness of neural networks&lt;/em&gt;&lt;/a&gt; [1]
&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;obfuscated-gradients&#34;&gt;Obfuscated Gradients&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Athalye, A., Carlini, N., &amp;amp; Wagner, D. &amp;ldquo;Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.&amp;rdquo; ArXiv e-prints, arXiv: 1802.00420. February 2018.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Optimization-based attacks such as the Basic Iterative Method, Projected Gradient Descent, and Carlini and Wagner’s attack are powerful threat to most defenses against adversarial examples in machine learning classifiers. New defenses have been proposed that appear to be resistant to optimization-based attacks, but Athalye, Carlini, and Wagner argue that these defenses are not as robust as they claim.&lt;/p&gt;

&lt;p&gt;Optimization-based attacks generate adversarial examples using gradients obtained through backpropagation. The authors identify obfuscated gradients as an element of many defenses that provides a facade of security by causing the attacker’s gradient descent to fail. Obfuscated gradients come in at least three forms: shattered gradients, stochastic gradients, and vanishing/exploding gradients.&lt;/p&gt;

&lt;p&gt;A defense uses &lt;strong&gt;shattered gradients&lt;/strong&gt; when it introduces non-differentiable operations, numeric instability, or otherwise causes the attacker’s gradient signal to be incorrect. A defense uses &lt;strong&gt;stochastic gradients&lt;/strong&gt; when the inputs to the classifier are randomized or a stochastic classifier is used, resulting in a different gradient each time it is evaluated. &lt;strong&gt;Vanishing/exploding gradients&lt;/strong&gt; are issues encountered in training some networks in which the gradient grows or shrinks exponentially during backpropagation. Some defenses involve multiple iterations of neural network evaluation. This can be viewed as evaluating one very deep neural network, which obfuscates the gradient signal by forcing the vanishing/exploding gradient problem.&lt;/p&gt;

&lt;h3 id=&#34;detecting-obfuscated-gradients&#34;&gt;Detecting Obfuscated Gradients&lt;/h3&gt;

&lt;p&gt;The authors propose a number of tests that might help detect when a defense relies on obfuscated gradients.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Iterative attacks should work better than single-step attacks, since iterative attacks are strictly stronger than single-step attacks.&lt;/li&gt;
&lt;li&gt;White-box attacks should perform better than black-box attacks, since the black-box threat model is a strict subset of the white-box threat model.&lt;/li&gt;
&lt;li&gt;Attacks with an unbounded distortion metric (e.g. \(L_2\) norm) should find adversarial examples with 100% success.&lt;/li&gt;
&lt;li&gt;Optimization-based attacks should perform better than brute-force sampling of nearby inputs (sampling within an \(\epsilon\)-ball).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These tests may not cover all cases of obfuscated gradients, but they indicate when intuitive properties start to break down. All defenses with obfuscated gradients discussed by the authors fail at least one test.&lt;/p&gt;

&lt;h3 id=&#34;attack-techniques&#34;&gt;Attack Techniques&lt;/h3&gt;

&lt;p&gt;The authors introduce attack techniques to address each of the three types of obfuscated gradients.&lt;/p&gt;

&lt;p&gt;Shattered gradients can be attacked using a technique called Backward Pass Differentiable Approximation. For a classifier \(f(\cdot)\), some defenses make gradients unavailable to attackers by incorporating a non-differentiable preprocessing step \(g(\cdot)\), making the secured classifier \(\hat{f}(\cdot) = f(g(\cdot))\). Backward Pass Differentiable Approximation estimates gradients by substituting a smooth, differentiable approximation of \(g\).&lt;/p&gt;

&lt;p&gt;Stochastic gradient-based defenses that randomly transform inputs can be defeated using the Expectation over Transformation technique to compute the gradient over the expected transformation of the input. If a stochastic classifier is used, the gradient can be similarly computed over the expectation of random parameters.&lt;/p&gt;

&lt;p&gt;Defenses that use vanishing/exploding gradients can be circumvented by reparameterizing the secured classifier and optimizing over a space where vanishing/exploding gradients do not occur.&lt;/p&gt;

&lt;p&gt;Seven of the eight defense techniques accepted to ICLR 2018 are based on a form of obfuscated gradients, and are therefore vulnerable to these attacks. The defense by Madry, et al., discussed below, was the only approach evaluated that doesn’t cause obfuscated gradients. Athalye, Carlini, and Wagner recommend that future defenses should be presented with specific, realistic threat models, testable claims with bounded test parameters, and evaluations against new defense-aware attacks.&lt;/p&gt;

&lt;h2 id=&#34;resistance-to-adversarial-attacks&#34;&gt;Resistance to Adversarial Attacks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. &lt;em&gt;Towards Deep Learning Models Resistant to Adversarial Attacks&lt;/em&gt;.  ICLR 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1706.06083.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.06083.pdf&#34;&gt;Madry et al.&lt;/a&gt; propose a general framework to study the defense of deep learning models against adversarial attacks.  The goal of their framework is to provide precise security guarantees about how a particular defense stacks up against entire classes of attacks.  Specifically, they offer the following saddle point problem:&lt;/p&gt;

&lt;p&gt;$$\underset{\theta}{\text{min}} \, \rho(\theta), \quad \text{where} \quad \rho(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}} \bigg[ \underset{\delta \in \mathcal{S}}{\text{max}} \, L(\theta, x + \delta, y) \bigg] \, .$$&lt;/p&gt;

&lt;p&gt;Let’s break this down:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\( \theta \) is the set of parameters of the model.  The attacker aims to exploit an existing model with a predetermined \( \theta \), and the defender can tune \( \theta \) as they wish.&lt;/li&gt;
&lt;li&gt;\( x \) and \( y \) are the example and label.&lt;/li&gt;
&lt;li&gt;\( L \) is the loss function.&lt;/li&gt;
&lt;li&gt;\( \mathbb{E} \) is a risk function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a classic saddle point problem, consisting of two connected parts: an inner maximization problem, and an outer minimization problem. An adversary seeks to find an example that maximizes the model’s loss (the inner problem), and the defender seeks to tune their model such that the potential loss is minimized (the outer problem).  Therefore, solving this problem (by minimizing rho) maximizes the robustness of a deep learning model.&lt;/p&gt;

&lt;p&gt;The inner loss function was modeled with a fast gradient sign method (FGSM) attack. The FGSM perturbed data was used for as training data on the defense side. The outer minimization function was a little more complicated. It’s not simple enough to train on FGSM adversaries. We want something that’s universally robust, so we use PGD, a first-order method to solve constrained optimization problems. The authors used PGD to find local maxima, since they are the areas of highest loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/Madry1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These graphs show how the loss scales as the number of PGD iterations increases.  The loss grows and plateaus as the number of iterations grows (as expected for any model).  However, the adversarially-trained models display much less loss than the naturally-trained models, indicating that Madry et. al.’s defense works well against their attacks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://secml.github.io/images/class3/Madry2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These graphs show the distributions of loss values, for various starting points.  The loss values are obtained by randomly sampling points within an \(L\infty\)-ball centered at the starting point, then using PGD to find the local maxima near those random points.  Although the locations of the points are widely distributed, the loss values are all clustered together.  The authors claim that these graphs display the universality of PGD: any local maxima with significantly higher losses would likely be infeasible for any other first-order method to find.&lt;/p&gt;

&lt;p&gt;— Team Gibbon: &lt;br /&gt;
Austin Chen, Jin Ding, Ethan Lowman, Aditi Narvekar, Suya&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1608.04644.pdf&#34;&gt;[1]&lt;/a&gt; Carlini N, Wagner D. Towards evaluating the robustness of neural networks. InSecurity and Privacy (SP), 2017 IEEE Symposium on 2017 May 22 (pp. 39-57). IEEE.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6980.pdf&#34;&gt;[2]&lt;/a&gt; Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 2014 Dec 22.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;[3]&lt;/a&gt; Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572. 2014 Dec 20.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.02533.pdf&#34;&gt;[4]&lt;/a&gt; Kurakin A, Goodfellow I, Bengio S. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533. 2016 Jul 8.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.04599.pdf&#34;&gt;[5]&lt;/a&gt; Moosavi Dezfooli SM, Fawzi A, Frossard P. Deepfool: a simple and accurate method to fool deep neural networks. InProceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016 (No. EPFL-CONF-218057).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.07528.pdf&#34;&gt;[6]&lt;/a&gt; Papernot N, McDaniel P, Jha S, Fredrikson M, Celik ZB, Swami A. The limitations of deep learning in adversarial settings. InSecurity and Privacy (EuroS&amp;amp;P), 2016 IEEE European Symposium on 2016 Mar 21 (pp. 372-387). IEEE.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.04508.pdf&#34;&gt;[7]&lt;/a&gt; Papernot N, McDaniel P, Wu X, Jha S, Swami A. Distillation as a defense to adversarial perturbations against deep neural networks. InSecurity and Privacy (SP), 2016 IEEE Symposium on 2016 May 22 (pp. 582-597). IEEE.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf&#34;&gt;[8]&lt;/a&gt; A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, &amp;ldquo;Towards Deep Learning Models Resistant to Adversarial Attacks&amp;rdquo;.  February 2018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class 2: Privacy in Machine Learning</title>
      <link>https://secml.github.io/class2/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class2/</guid>
      <description>

&lt;p&gt;In today’s post we introduce some key concepts crucial to understanding the current state of privacy in machine learning. In a time where novel machine learning applications are seemingly announced weekly, privacy is becoming more relevant as learning algorithms play varied and sometimes critical roles in our lives. We introduce differential privacy and common ‘solutions’ that fail to protect individual privacy, explore membership inference attacks on blackbox machine learning models, and discuss a case study involving privacy in the field of pharmacogenetics, where machine learning models are used to guide patient treatment.&lt;/p&gt;

&lt;h2 id=&#34;membership-inference-attacks&#34;&gt;Membership inference attacks&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. &lt;em&gt;Membership Inference Attacks Against Machine Learning Models&lt;/em&gt;. IEEE Symposium on Security and Privacy (&amp;ldquo;Oakland&amp;rdquo;) 2017. [&lt;a href=&#34;https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf&#34;&gt;Shokri et al.&lt;/a&gt; attempt to attack black box machine learning models based on subtle data leaks based on the outputs. If the membership of a datapoint can be identified in the training set of a black box machine, it poses a significant privacy risk to the data of users of machine learning services. This is especially important for machine learning services such as Google Prediction and Amazon ML, as an inability to guarantee the privacy of training data would preclude a significant number of customers from using their services.&lt;/p&gt;

&lt;p&gt;The attack is based on the idea that there are noticeable patterns in outputs when the input is given to the model as training data. While no human would be able to pick up on a pattern of that nature, a machine learning model with a theoretically limitless data source has that power. As the purpose of machine learning is to notice and evaluate patterns beyond human recognition, using a machine learning model to attack black box machine learning models makes sense.&lt;/p&gt;

&lt;p&gt;For a machine learning model to attack the black box, it first needs to train against other models where it can verify its own accuracy. For this, similar models need to be created for evaluation. These models are called shadow models, and need to be trained on similar sample data to the original model. To generate training data, these models use hill climbing methods on the black box to seek high confidence areas. By then sampling data using a simple distribution about those points, somewhat accurate training sets can be created. Ideally, these sets will be roughly the same size and distribution of the target model.&lt;/p&gt;

&lt;p&gt;The attack model is then trained on a set of shadow models, using classifications based on whether or not the datapoint was in the training set of the shadow model. This theoretically works based on the attack model being able to locate areas in which the curves are overfitted in the shadow models and translate that into membership. Ultimately, the study produced results that ranged from 70% to 95% precision overall based on the target model. The model gave nearly no false negatives.&lt;/p&gt;

&lt;p&gt;To limit the power of these attacks, more regularization can be used to minimize overfitting, as the overfitting patterns become less apparent as the curve fitting is minimized. However, differential privacy is the more general class of solutions that is the defining foundation of the current state of the field of privacy in machine learning.&lt;/p&gt;

&lt;h2 id=&#34;differential-privacy&#34;&gt;Differential privacy&lt;/h2&gt;

&lt;p&gt;With the explosion of data collection from various social platforms and the increasing usage of machine learning on personalized applications like personalized medication, medical diagnostics, genomics and face recognition, the collection of sensitive human data is inevitable. When dealing with personal information, it is paramount to preserve the privacy of any individual participating in the data set. In a word, it is necessary to ensure that an adversary does not learn about the presence or absence of any individual’s information from looking at the synopsis of the data set. This goal is the central motivation the concept of differential privacy.&lt;/p&gt;

&lt;p&gt;To fully appreciate differential privacy, let us discuss some &lt;a href=&#34;http://www.lrdc.pitt.edu/schunn/cdi2009/presentations/Dwork.pdf&#34;&gt;alternative solutions&lt;/a&gt; to preserving individual privacy in a data set identify how each one fails in achieving the goal of privacy.&lt;/p&gt;

&lt;h4 id=&#34;failed-alternatives&#34;&gt;Failed alternatives&lt;/h4&gt;

&lt;h6 id=&#34;1-allowing-only-group-queries&#34;&gt;1. Allowing only group queries&lt;/h6&gt;

&lt;p&gt;It seems intuitive that individual data privacy can be preserved by restricting a user to only group queries. For example, a group query can be: “How many students in this class earned an A?”. But this blatantly violates the privacy if the user also queries “How many students in this class apart from Rick earned an A?”, where the end user can know if Rick received an A or not based on the answer to the two queries.&lt;/p&gt;

&lt;h6 id=&#34;2-add-random-noise-to-the-query-result&#34;&gt;2. Add random noise to the query result&lt;/h6&gt;

&lt;p&gt;Adding random noise to the query result tends to obfuscate the relation between the input and the output, but an adversary can determine the noise distribution by repeated querying. Hence, this approach also fails.&lt;/p&gt;

&lt;h6 id=&#34;3-deterministic-perturbation-in-answering&#34;&gt;3. Deterministic perturbation in answering&lt;/h6&gt;

&lt;p&gt;We can deterministically map every output to another value in the vector space to obfuscate the input-output relation. But this becomes far too computationally complex in a high-dimensional regime.&lt;/p&gt;

&lt;h6 id=&#34;4-withholding-sensitive-information&#34;&gt;4. Withholding sensitive information&lt;/h6&gt;

&lt;p&gt;An easy, straightforward notion is to simply withhold any sensitive data before executing any query; however, this very action reveals information about the presence or absence of sensitive data.&lt;/p&gt;

&lt;h6 id=&#34;5-make-output-independent-of-any-individual-record&#34;&gt;5. Make output independent of any individual record&lt;/h6&gt;

&lt;p&gt;This approach certainly does not reveal the presence or absence of an individual in the data set. But this method is not of any practical use since, by mathematical induction, the output does not depend on the data set itself.&lt;/p&gt;

&lt;h6 id=&#34;6-k-anonymity&#34;&gt;6. K-anonymity&lt;/h6&gt;

&lt;p&gt;Another way to achieve obfuscation of a record is to group it with \(k\) other records for each attribute. Unfortunately, this method also fails in high-dimensional cases, where it has been shown to be feasible to still distinguish an individual record.&lt;/p&gt;

&lt;h6 id=&#34;7-semantic-security&#34;&gt;7. Semantic Security&lt;/h6&gt;

&lt;p&gt;Semantic security is a strict notion in cryptography which ensures that the advantage of an adversary with an auxiliary information should be cryptographically small with or without the access to the output (ciphertext). In our query model, this definition is too strong and does not allow learning any useful information from the data set.&lt;/p&gt;

&lt;h4 id=&#34;defining-differential-privacy&#34;&gt;Defining differential privacy&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;formal definition&lt;/a&gt; of differential privacy is given as:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A randomized computation \(M\) satisfies \(\epsilon\)-differential privacy if for any adjacent data sets \(x\) and \(x’\), and any subset \(C\) of possible outcomes \(Range(M)\),&lt;/p&gt;

&lt;p&gt;$$Pr[M(x) \in C] \leq \exp(\epsilon) \times Pr[M(x&amp;rsquo;) \in C]$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In other words, this means that the chance that an event occurs &lt;em&gt;with your data&lt;/em&gt; and the chance it would occur &lt;em&gt;without your data&lt;/em&gt; is closely bounded by a privacy budget \(\epsilon\). The figure below depicts the equation where the two lines denote the probability distribution over \(x\) and \(x’\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/secML/secML.github.io/master/src/content/images/img1.png&#34; alt=&#34;&#34; title=&#34;Response probability distribution&#34; /&gt;
   &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(http://www.lrdc.pitt.edu/schunn/cdi2009/presentations/Dwork.pdf)&#34;&gt;&lt;em&gt;Differential Privacy and Pan-Private Algorithms&lt;/em&gt;&lt;/a&gt; [6]
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;There is another relaxed version of the above definition of differential privacy, called \((\epsilon,\delta)\)-differential privacy, &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;defined&lt;/a&gt; as below:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A randomized computation \(M\) satisfies \((\epsilon,\delta)\)-differential privacy if for any adjacent data sets \(x\) and \(x’\), and any subset \(C\) of possible outcomes \(Range(M)\),&lt;/p&gt;

&lt;p&gt;$$Pr[M(x) \in C] \leq \exp(\epsilon) \times Pr[M(x&amp;rsquo;) \in C] + \delta$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The above definition basically implies that with probability \((1-\delta)\), \(M\) preserves \(\epsilon\)-differential privacy.&lt;/p&gt;

&lt;h4 id=&#34;properties-of-differential-privacy&#34;&gt;Properties of differential privacy&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Group Privacy&lt;/strong&gt;: An \(\epsilon\)-differentially private algorithm can be extended to provide group privacy for a group size of \(k\) by scaling the privacy budget to \(k\epsilon\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Composition&lt;/strong&gt;: Composition of \(k \epsilon\)-differential privacy and \((\epsilon,\delta)\)-differential privacy methods leads to \(k \epsilon\)-differential privacy and \((k \epsilon, k \delta)\)-differential privacy methods respectively.&lt;/p&gt;

&lt;h4 id=&#34;differential-privacy-with-laplace-noise&#34;&gt;Differential privacy with Laplace noise&lt;/h4&gt;

&lt;p&gt;To ensure \(\epsilon\)-differential privacy for a function \(f(x)\), we can add &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;Laplace noise&lt;/a&gt; to it:&lt;/p&gt;

&lt;p&gt;$$ f(x) + \text{Laplace}(0, \sigma)^d $$&lt;/p&gt;

&lt;p&gt;where \(\Delta f = \text{max} || f(x)-f(x&amp;rsquo;) ||_1 \)  and \(\sigma \geq \frac{\Delta f}{\epsilon}\).&lt;/p&gt;

&lt;p&gt;Let us consider a working example. Consider that there are two adjacent data sets \(D\) and \(D&amp;rsquo;\) consisting of scores of students of a class, such that \(D\) and \(D’\) differ by only one student’s record.&lt;/p&gt;

&lt;p&gt;Consider the function \(f\) that computes the class average. If, say, the minimum and maximum attainable scores are \(0\) and \(100\) respectively, then the sensitivity of \(f\) is \( \frac{100-0}{4} = 25\).&lt;/p&gt;

&lt;p&gt;Thus, to release the class average of \(D\) is given by
\(\frac{65+83+77+56}{4}\) with \(\epsilon\)-differential privacy,
we would add the result of sampling \(\text{Laplace}(0,
\frac{25}{\epsilon})\) to the computed value. This preserves the
privacy of the student differing in \(D\) and \(D’\).&lt;/p&gt;

&lt;h2 id=&#34;privacy-in-pharmacogenetics&#34;&gt;Privacy in Pharmacogenetics&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Matthew Fredrikson, Eric Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas
&lt;em&gt;Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing&lt;/em&gt;. USENIX Security Symposium 2014. [&lt;a href=&#34;https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In their case study, &lt;a href=&#34;https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf&#34;&gt;Fredrikson et al.&lt;/a&gt;
analyze personalized Warfarin doses, a widely used anticoagulant. Finding the correct dose of Warfarin is vitally important as both low and high doses may result in death of the patient &amp;ndash; literally, a matter of life and death.&lt;/p&gt;

&lt;p&gt;This experiment was done on the available IWPC dataset which contains and uses demographic information &lt;em&gt;(age, height, weight, race)&lt;/em&gt;, genetic markers &lt;em&gt;(CYP2C9/VKORC1)&lt;/em&gt; and clinical histories of people around the world to predict their appropriate Warfarin dose. The IWPC has found that linear regression is the best learning model to fit the data and has made the model available for physicians&amp;rsquo; use in calculating the initial dose of the Warfarin.&lt;/p&gt;

&lt;p&gt;Let’s highlight how an adversary can use model inversion attack to violate the genomic privacy of a patient. We first initially assume that an adversary has black-box access to the trained model. As it turns out, due to the highly linear nature of the data, by running the model backwards, the sensitive genotype can be predicted given the stable dose of Warfarin, some basic demographic facts regarding the patient, and the marginal priors of the patient distribution.&lt;/p&gt;

&lt;p&gt;How does the attack work? First, the adversary computes all the values of the missing variables that could potentially agree with the given information. Then, the adversary runs the model forward for each hypothetical patient in the dataset to predict the stable Warfarin doses. Finally, the adversary performs a likelihood computation to find out which configuration of the missing variables are most probable. Given the information and model inversion setting, this algorithm is optimal, as it minimizes the adversaries misprediction rate. With respect to the baseline, the accuracy of the model inversion attack is only 5% lower in comparison to ideal prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/secML/secML.github.io/master/src/content/images/img2.png&#34; alt=&#34;&#34; title=&#34;Model inversion attack&#34; /&gt;
   &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;https://www.usenix.org/sites/default/files/conference/protected-files/sec14_slides_fredrikson.pdf&#34;&gt;Privacy in Pharmacogenetics presentation&lt;/a&gt; [2]
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;As seen in this image, the attacker computes the values of missing variables, then predicts the Warfarin doses; and finally, using this information, finds the most likely configuration.&lt;/p&gt;

&lt;p&gt;Adding noise using differential privacy strategy can be a countermeasure against the model inversion attack. But using differential privacy decreases the utility of the trained model. Based on their simulated trials, authors claim that there is no such privacy budget that can prevent model inversion, without introducing  risk of overfixed dosing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/secML/secML.github.io/master/src/content/images/img3.png&#34; alt=&#34;&#34; title=&#34;Privacy budget plot&#34; /&gt;
   &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;https://www.usenix.org/sites/default/files/conference/protected-files/sec14_slides_fredrikson.pdf&#34;&gt;Privacy in Pharmacogenetics presentation&lt;/a&gt; [2]
   &lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-netflix-prize-semi-supervised-deep-learning-from-private-training-data&#34;&gt;The Netflix Prize: Semi-supervised deep learning from private training data&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Frank McSherry and Ilya Mironov, &lt;em&gt;Differentially Private Recommender Systems: Building Privacy into the Netflix Prize Contenders&lt;/em&gt;. KDD 2009. [&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Traditional recommender systems are usually not designed with an emphasis on privacy, which can be detrimental when the adversary are able to create multiple accounts to affect the recommendation at will. &lt;a href=&#34;https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf&#34;&gt;Researchers&lt;/a&gt; have been successful in developing powerful attack linking records in the Netflix Prize data set with other public user data. The implication is that sensitive information as the input of a recommender system can be linked and made inference about. &lt;a href=&#34;https://www.cs.utexas.edu/~shmat/shmat_oak11ymal.pdf&#34;&gt;Similar practical examples&lt;/a&gt; include making inference about purchase history through Amazon’s recommendations.&lt;/p&gt;

&lt;p&gt;McSherry and Mironov’s &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;paper&lt;/a&gt; adapted the leading recommendation algorithms (factor models and neighborhood models) to the differential privacy framework in order to develop a recommender system based on the Netflix data set while providing privacy guarantees. As mentioned in the above section, differential privacy enables privacy preserving computation by substantially precluding inference from the output data, unlike the prior efforts focusing mainly on cryptographic solutions that limit access to data of users. By doing so, it is conceivable that more uncertainty/noise is added to computation.&lt;/p&gt;

&lt;p&gt;One interesting aspect this paper dealt with is the privacy vs. accuracy tradeoff. Evaluation of their approach was applied to the Netflix Prize data set which bears extremely high dimensionality. The root mean squared error (RMSE) was used as the metric for accuracy. The findings are as the quality of privacy increases, the accuracy of the recommendation drops.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/secML/secML.github.io/master/src/content/images/img4.png&#34; alt=&#34;&#34; title=&#34;RMSE for four algorithms as a function of θ \\(\propto\\\) 1/σ on the Netflix Prize set.&#34; /&gt;
   &lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf)&#34;&gt;&lt;em&gt;Differentially Private Recommender Systems&lt;/em&gt;&lt;/a&gt; [3]
   &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Privacy vs. accuracy over time was also studied to explore how the loss of accuracy due to privacy-preserving decreased as more data become available for a fixed value of the privacy parameter. The results are summarized in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/secML/secML.github.io/master/src/content/images/img5.png&#34; alt=&#34;&#34; title=&#34;Left scale—accuracy loss, right scale—the
number of records. The \\(x\\\)-axis is the number of days
elapsed since 7/1/2000.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;caption&#34;&gt;
Source: &lt;a href=&#34;(https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf)&#34;&gt;&lt;em&gt;Differentially Private Recommender Systems&lt;/em&gt;&lt;/a&gt; [3]
   &lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have introduced differential privacy and formally defined what is meant by preservation of \(\epsilon\)-differential privacy; discussed common solutions and how they fail; explained the properties of a differentially private algorithm, and how adding Laplace Noise enables \(\epsilon\)-differential private functions. We also discussed how in Membership Inference Attacks Against Machine Learning Models, Shokri et al. discuss attacking a black box model by training on a set of shadow models. Finally,  we discussed how in Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing Fredrikson et al. demonstrate how a model inversion attack can be employed to infer patient genotype information.&lt;/p&gt;

&lt;p&gt;— Team Nematode: &lt;br /&gt;
Bargav Jayaraman, Guy &amp;ldquo;Jack&amp;rdquo; Verrier, Joshua Holtzman, Max Naylor, Nan Yang, Tanmoy Sen&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf&#34;&gt;[1]&lt;/a&gt; R. Shokri, M. Stronati, C. Song, V. Shmatikov, &amp;ldquo;Membership Inference Attacks Against Machine Learning Models.&amp;rdquo; May 2017.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf&#34;&gt;[2]&lt;/a&gt; M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, T. Ristenpart, &amp;ldquo;Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.&amp;rdquo; August 2014.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/NetflixPrivacy.pdf&#34;&gt;[3]&lt;/a&gt; F. McSherry, I. Mironov, &amp;ldquo;Differentially Private Recommender Systems: Building Privacy into the Netflix Prize Contenders.&amp;rdquo; June 2009.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf&#34;&gt;[4]&lt;/a&gt; A. Narayanan, V. Shmatikov, &amp;ldquo;Robust De-anonymization of Large Sparse Datasets.&amp;rdquo; May 2008.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.utexas.edu/~shmat/shmat_oak11ymal.pdf&#34;&gt;[5]&lt;/a&gt; J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, V. Shmatikov, &amp;ldquo;&amp;lsquo;You Might Also Like&amp;rsquo;: Privacy Risks of Collaborative Filtering.&amp;rdquo; May 2011.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.lrdc.pitt.edu/schunn/cdi2009/presentations/Dwork.pdf&#34;&gt;[6]&lt;/a&gt; C. Dwork, &amp;ldquo;Differential Privacy and Pan-Private Algorithms.&amp;rdquo; August 2016.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OfWj89oRD7g&#34;&gt;[7]&lt;/a&gt; C. Dwork, &amp;ldquo;Differential Privacy - Lecture 1&amp;rdquo;. August 2016.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class 1: Intro to Adversarial Machine Learning</title>
      <link>https://secml.github.io/class1/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/class1/</guid>
      <description>

&lt;h2 id=&#34;machine-learning-background&#34;&gt;Machine Learning Background&lt;/h2&gt;

&lt;!-- - Other ML algorithms use linear decision boundaries (SVM, LR, ...)
- Deep learning uses linear units with nonlinear composition
    - More susceptible to attacks
- Define terms like gradient and loss --&gt;

&lt;p&gt;In supervised Machine Learning, we train models with training data
along with the label associated with it. We extract features from each
sample, and use an algorithm to train a model where the inputs are
those features and the output is the label.&lt;/p&gt;

&lt;p&gt;For classifying the testing data, the classifier uses decision
boundary to separate points of the data belonging to each class. In a
statistical-classification problem with two classes, a decision
boundary partitions all the underlying vector space into two separate
classes. A support vector machine (SVM) is a linear classifier which
constructs a boundary by focusing two closest points from different
classes and it finds the line that is equidistant to these two points.&lt;/p&gt;

&lt;h4 id=&#34;loss-functions&#34;&gt;Loss Functions&lt;/h4&gt;

&lt;p&gt;A loss function define that how good a given model is at making
predictions for a given scenario. It has its own curve and its own
gradients. The slope of the curve indicates the appropriate way of
updating the parameters to make the model more accurate in case of
prediction. A frequently used loss function is the 0-1 loss function.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/01_loss_function.png&#34; width=&#34;150&#34; &gt;
&lt;/p&gt;

&lt;p&gt;where \(I\) is the indicator notation. Hinge loss function is also popular in machine learning field. It provides a relatively tight, convex upper bound on the 0-1 indicator function.
s
&lt;!-- &lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/hinge_function.png&#34; width=&#34;150&#34; &gt;
&lt;/p&gt; --&gt;&lt;/p&gt;

&lt;h4 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;Gradient Descent is an optimization algorithm which is used to minimize cost function by iteratively moving the direction of the steepest descent. In machine learning, we use gradient descent to update the parameters (coefficients in Linear Regression and weight in neural networks) of our model.&lt;/p&gt;

&lt;p&gt;&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/gradient_descent_graph.png&#34; width=&#34;300&#34; &gt;
&lt;br&gt;
Image credit: &lt;a href=&#34;https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html&#34;&gt;Sebastian Raschka&lt;/a&gt;
   &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;In the figure, weight is slowly decreasing from the initial stage. \(J_{min}\) is the minimum value of the cost function which can be obtained by optimizing the algorithm.&lt;/p&gt;

&lt;p&gt;Recently, there have been many successful applications of Deep Neural
Networks (DNN) in the fields of information retrieval, computer
vision, and speech recognition. Despite their potential, deep neural
architectures, like all other machine learning approaches, are
vulnerable to what are known as adversarial samples. These systems can
be fooled by targeted manipulations which slightly modifying a real
example to trick the model into “believing” that this modified sample
belongs to an incorrect class with high confidence. To defend against
these adversarial attacks, many researchers are exploring several
directions including data augmentation, increasing model complexity,
retraining, pre-processing, etc. Their main goal is to protect the
model against adversarial manipulations by the attackers.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;applications-and-vulnerabilities&#34;&gt;Applications and Vulnerabilities&lt;/h2&gt;

&lt;p&gt;To get a better idea of what security and privacy mean in a machine learning context, we highlight a few
applications and examine the possible consequences of vulnerabilities in these domains.&lt;/p&gt;

&lt;p&gt;One of the most commonly-discussed applications is in self-driving
cars, which use machine learning algorithms for image recognition
tasks, such as identifying traffic signs and road hazards.  If an
attacker were to cause an erroneous classification, e.g. mistaking a
stop sign for a speed limit sign or missing a pedestrian in a
crosswalk, both property and human lives would be in
jeopardy. (Although luckily self-driving cars use a variety of other
techniques to avoid collisions, including detailed maps and LIDAR, so
are likely to not be directly vulnerable to image mis-classification
attacks.)&lt;/p&gt;

&lt;p&gt;Machine learning has also found a place in the medical world, being used to identify patterns and trends in patient histories that can be used for diagnoses, to recognize abnormalities in medical imaging, and even to evaluate the efficiency of a hospital&amp;rsquo;s workflow.  The use of machine learning in healthcare introduces a new concern - how can patient data be effectively utilized for beneficial purposes, like predicting appropriate drug dosages, without compromising patient privacy?&lt;/p&gt;

&lt;p&gt;There are numerous other uses of ML today (targeted advertising, fraud detection, malware detection, etc.) and each carries particular security and privacy concerns.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;adversarial-examples&#34;&gt;Adversarial Examples&lt;/h2&gt;

&lt;p&gt;In machine learning, an adversarial example is an input that has been manipulated so that the model returns a different, incorrect output.  A classic adversarial example for image classification is from a paper by &lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;Goodfellow et al.&lt;/a&gt;, which features a photo of a panda that was originally classified correctly, but is misclassified when carefully-crafted noise is added.  The model classifies the new image as a gibbon, even though human eyes can easily see it is a panda.&lt;/p&gt;

&lt;p&gt;&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/panda.png&#34; width=&#34;600&#34; &gt;
&lt;br&gt; Source: &lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;Goodfellow Paper&lt;/a&gt;
   &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;As outlined in &lt;a href=&#34;https://arxiv.org/pdf/1712.03141.pdf&#34;&gt;Biggio et al.&amp;rsquo;s&lt;/a&gt; paper on the history of adversarial machine learning, adversarial examples were first described in 2004 in the context of email spam filters. At this time, &lt;a href=&#34;https://homes.cs.washington.edu/~pedrod/papers/kdd04.pdf&#34;&gt;Dalvi et al.&lt;/a&gt; and &lt;a href=&#34;https://ix.cs.uoregon.edu/~lowd/kdd05lowd.pdf&#34;&gt;Lowd and Meek&lt;/a&gt; realized that slight modifications to spam emails allowed them to pass through the filters, without greatly affecting the content of the message. From this point, the field expanded to study the potential for adversarial examples in other realms, like machine-learning-based image and malware classification.  The increased usage of deep learning techniques for object recognition led to a surge in interest around 2014, when Szegedy et al. showed that deep convolutional neural networks were also susceptible to adversarial examples.  Since then, interest in this field has only continued to increase, with more and more papers published each year.&lt;/p&gt;

&lt;h3 id=&#34;classifying-attacks&#34;&gt;Classifying Attacks&lt;/h3&gt;

&lt;p&gt;Attacks can be categorized by many different characteristics, but are often referred to in terms of the attack method, the goal of the attack, and the adversary&amp;rsquo;s level of knowledge, as detailed in &lt;a href=&#34;https://arxiv.org/pdf/1611.03814.pdf&#34;&gt;Papernot et al.&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;attack-method&#34;&gt;Attack Method&lt;/h4&gt;

&lt;p&gt;A poisoning attack is when the adversary adds carefully-crafted samples into the training data, thereby disrupting the learning process and manipulating the final trained model.  An evasion attack occurs when a sample originally correctly classified into class A is manipulated and fed back into the model, now receiving an output that is &lt;em&gt;not&lt;/em&gt; class A.  A key difference between these two attacks is where each occurs in the ML pipeline: poisoning attacks occur before the training stage and evasion attacks occuring after training, during the testing stage.&lt;/p&gt;

&lt;p&gt;&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/ml_pipeline.jpg&#34; width=&#34;650&#34; &gt;
&lt;br&gt; Source: &lt;a href=&#34;https://docs.google.com/presentation/d/1dFRjRfCIz1TChZYfIY_FLYiSk7nNmdBY2luJ8T_TCAE/edit?usp=sharing&#34;&gt;Presentation Slides&lt;/a&gt;
   &lt;/p&gt;&lt;/p&gt;

&lt;h4 id=&#34;goal-of-attack&#34;&gt;Goal of Attack&lt;/h4&gt;

&lt;p&gt;The goal of the adversary is another way to categorize attacks, which
in this case means whether or not there was a specific goal
classification or not.  &lt;em&gt;Error-generic&lt;/em&gt; (also called &lt;em&gt;untargeted&lt;/em&gt;)
attacks aim to find a sample close to a given seed that is
misclassified, but do not have a specific target output
class. &lt;em&gt;Error-specific&lt;/em&gt; (&lt;em&gt;targeted&lt;/em&gt;) attacks deliberately change the
seed sample&amp;rsquo;s classification from the original class A to a chosen
class B.&lt;/p&gt;

&lt;h4 id=&#34;adversary-knowledge&#34;&gt;Adversary Knowledge&lt;/h4&gt;

&lt;p&gt;The level of knowledge the adversary possesses generally falls into one of three loosely defined categories: black box, grey box, and white box attacks.  Black box attacks assume the lowest level of adversary knowledge, generally only allowing the final classification to be known without any finer details about the model.  On the other end of the spectrum, white box attacks assume that the attacker knows everything about the model, including the specific algorithm used, the feature set, the training data, etc. An attack by an adversary who knows more about the model than a black box but less than a white box is called a grey box attack, although the exact definition tends to vary.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/boxes.png&#34; width=&#34;600&#34; &gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;modern-machine-learning-as-a-potemkin-village&#34;&gt;Modern Machine Learning as a Potemkin village&lt;/h2&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/potemkin.jpeg&#34; width=&#34;350&#34; &gt;
&lt;/p&gt;

&lt;p&gt;The term &amp;ldquo;Potemkin village&amp;rdquo; describes a difference between appeance and reality. In a &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;paper&lt;/a&gt; by Goodfellow, Shlens, and Szegdy, modern machine learning methods are described as building &amp;ldquo;a Potemkin village that works well on naturally occuring data, but is exposed as a fake when one visits points in space that do not have high probability in the data distribution.&amp;rdquo; The apparent fragility of deep neural networks has been particuarly exposed, as seen in the panda picture above.&lt;/p&gt;

&lt;p&gt;The two-faced nature of machine learning models to be both fragile and robout from two different angles leads to concerns in the security of machine learning algorithms. High confidence in classifying adversarial examples poses a threat to the integrity of the model&amp;rsquo;s output.&lt;/p&gt;

&lt;h4 id=&#34;fast-gradient-sign-method-fgsm&#34;&gt;Fast gradient sign method (FGSM)&lt;/h4&gt;

&lt;p&gt;Also the &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;paper&lt;/a&gt; mentioned a method for generating adversarial examples. Fast gradient sign method maximizes the error between the ground truth classification of the sample and minimizes the error to the target classification. The method is effecitve is generating the best pixels to change to achieve a higher loss, essentially acting as a reverse optimization method. FGSM simply traverses the loss curve by moving in the opposite direction of the gradient loss.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/formula.png&#34; width=&#34;200&#34; &gt;
&lt;/p&gt;

&lt;p&gt;As we discussed, the sign method in the FGSM is an approximation to the actual gradient. Although the most accurate method would use the true gradient, the sign of the gradient is taken for the sake efficiency: since \(\epsilon\) reflect the adversarial strength (the maximum size perturbation allowed), this represents taking the largest step possible in each dimension in the direction given by the gradient. (There are many variations on FGSM that we&amp;rsquo;ll discuss in later posts, including iterative versions where a sequence of smaller steps is used.) FGSM showcases the fragility of machine learning models especially through visualizations such as the panda example.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;adversarial-training&#34;&gt;Adversarial training&lt;/h2&gt;

&lt;h4 id=&#34;effects-of-nonlinearities&#34;&gt;Effects of nonlinearities&lt;/h4&gt;

&lt;p&gt;One criticism of deep neural networks is that their nonlinearity creates vulnerabilities that can be exploited by adversarial examples.  As shown below, the mode function cuts through the data points linearly, while the function drawn by the neural network conforms more tightly to the training data points.  This overfitting to training data creates pockets in which the class chosen by the function does not match the ground truth class value for a given data point.&lt;/p&gt;

&lt;p&gt;&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;https://secml.github.io/images/capacity.png&#34; width=&#34;650&#34; &gt;
&lt;br&gt;Image credit: &lt;a href=&#34;https://www.papernot.fr/files/16-mcdaniel-sp-machine-learning-in-adversarial-settings.pdf&#34;&gt;McDaniel, Papernot, and Celik, &lt;em&gt;IEEE Security &amp;amp; Privacy Magazine&lt;/em&gt;&lt;/a&gt;
   &lt;/p&gt;&lt;/p&gt;

&lt;h4 id=&#34;solutions&#34;&gt;Solutions&lt;/h4&gt;

&lt;p&gt;As a possible solution, &lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;Szegedy et al.&lt;/a&gt; proposed incorporating adversarial examples into the training data.  Using various techniques to find the vulnerable pockets in the model, the authors were able to generate examples that would fall into those adversarial region.  Adding those samples to the training data results in a new model that does not follow the original training points as closely, creating a smoother, more accurate function.&lt;/p&gt;

&lt;p&gt;Using an adversarial &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34;&gt;regularizer&lt;/a&gt; is also a technique to limit overfitting and vulnerability to adversarial examples. &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;Goodfellow et al.&lt;/a&gt; describes a technique using the fast gradient sign method mentioned above as a regularizer. Their method generates adversarial examples using the fast gradient sign method and then trains the model with the adversarial examples. By continually updating the adversarial examples to the model, the adversarial regions are minimized. In the paper, they were able to reduce the error rate on adversarial examples from 0.94% to 0.84% using this approach. In order to reduce this error further, they also increased the model size and introduced early stopping on adversarial validation set error. Using these additional techniques they were able to drop the error rate on adversarial examples based on the FGSM from 89.4% to 17.9%. A combination of techniques introduced in training can significantly decrease the vunerability to simple adversarial attacks.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;transferability-of-adversarial-samples&#34;&gt;Transferability of adversarial samples&lt;/h2&gt;

&lt;p&gt;As shown above, an adversarial sample can fool a machine-learned model to misclassify it with high confidence. In particular, the adversarial samples made to fool image classifiers show how an adversarial sample can be indistinguishable to humans from legitimate samples. Additionally, examples that evade one classifier tend to evade others. This poses a vulnerability for machine learned models, one that can be exploited with cross-evasion.&lt;/p&gt;

&lt;h4 id=&#34;learning-classifier-substitutes&#34;&gt;Learning classifier substitutes&lt;/h4&gt;

&lt;p&gt;For example, &lt;a href=&#34;https://arxiv.org/abs/1605.07277&#34;&gt;Papernot et al.&lt;/a&gt;, took advantage of the transferability of adversarial examples on a remote DNN by training a new model with their own synthetic data. By using a relatively small number of queries to label their data, they trained a substitute model which they then used to craft adversarial samples. Demonstrating transferability, 84.24% of the adversarial samples trained from the substitute model fooled the remote DNN.&lt;/p&gt;

&lt;p&gt;To explicitly demonstrate the phenomenon of both intra and cross-technique transferability, &lt;a href=&#34;https://arxiv.org/abs/1605.07277&#34;&gt;Papernot et al.&lt;/a&gt; used five different machine learning algorithms on five disjoint training sets of MNIST dataset. With this setup, adversarial samples trained from one technique on one subset of the training data fools &amp;ndash; to varying degrees &amp;ndash; models trained by a different subset of the training data as well as a different technique. Furthermore, an ensemble classifier could be fooled at a rate of up to 44%.&lt;/p&gt;

&lt;p&gt;&amp;mdash; Team Panda:&lt;br /&gt;
Christopher Geier,
Faysal Hossain Shezan,
Helen Simecek,
Lawrence Hook,
Nishant Jha&lt;/p&gt;

&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6572.pdf&#34;&gt;[1]&lt;/a&gt; I.J. Goodfellow, J. Shlens, C. Szegedy, &amp;ldquo;Explaining
and Harnessing Adversarial Examples.&amp;rdquo; &lt;em&gt;ArXiv e-prints&lt;/em&gt;, December 2014.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1712.03141.pdf&#34;&gt;[2]&lt;/a&gt; B. Biggio, F. Roli, &amp;ldquo;Wild patterns: Ten years after the rise of
adversarial machine learning.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1712.03141&lt;/em&gt;, 2017.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~pedrod/papers/kdd04.pdf&#34;&gt;[3]&lt;/a&gt; N. Dalvi, P. Domingos, Mausam, S. Sanghai, D. Verma, &amp;ldquo;Adversarial classification,&amp;rdquo;&amp;rdquo; &lt;em&gt;Int’l Conf. Knowl. Disc. and Data Mining&lt;/em&gt;, 2004, pp. 99–108.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ix.cs.uoregon.edu/~lowd/kdd05lowd.pdf&#34;&gt;[4]&lt;/a&gt; D. Lowd, C. Meek, &amp;ldquo;Adversarial learning,&amp;rdquo; &lt;em&gt;Int’l Conf. Knowl. Disc. and Data Mining&lt;/em&gt;, ACM Press, Chicago, IL, USA, 2005, pp. 641–647.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.03814.pdf&#34;&gt;[5]&lt;/a&gt; N. Papernot, P. McDaniel, A. Sinha, M. Wellman, &amp;ldquo;Towards the science of security and privacy in machine learning.&amp;rdquo; &lt;em&gt;IEEE European Symposium on Security and Privacy&lt;/em&gt;,
2018.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.07277&#34;&gt;[6]&lt;/a&gt; N. Papernot, P. Mcdaniel, I.J. Goodfellow, &amp;ldquo;Transferability in machine learning: from phenomena to black-box attacks using adversarial samples&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:1605.07277&lt;/em&gt;, 2016.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;[7]&lt;/a&gt; C. Szegedy, W. Zaremba, I. Sutskever, J Bruna, D. Erhan, I.J. Goodfellow, R. Fergus. &amp;ldquo;Intriguing properties of neural networks.&amp;rdquo; &lt;em&gt;ICLR,&lt;/em&gt; abs/1312.6199, 2014.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Week</title>
      <link>https://secml.github.io/first-week/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/first-week/</guid>
      <description>&lt;p&gt;&lt;em&gt;This message was also sent out by email.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since not everyone has joined the slack yet, I&amp;rsquo;m sending this out by
email, but please make sure to join
&lt;a href=&#34;https://secprivml.slack.com&#34;&gt;https://secprivml.slack.com&lt;/a&gt; soon. I
will use that for future communications.&lt;/p&gt;

&lt;p&gt;I have grouped the 18 full participants in the class into three teams of six:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team Bus:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Anant Kharkar&lt;br /&gt;
Ashley Gao&lt;br /&gt;
Atallah Hezbor&lt;br /&gt;
Joshua Holtzman&lt;br /&gt;
Mainuddin Ahmad Jonas&lt;br /&gt;
Weilin Xu&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team Gibbon:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Aditi Narvekar&lt;br /&gt;
Austin Chen&lt;br /&gt;
Ethan Lowman&lt;br /&gt;
Guy &amp;ldquo;Jack&amp;rdquo; Verrier&lt;br /&gt;
Jialei Fu&lt;br /&gt;
Jin Ding&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Team Panda:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bargav Jayaraman&lt;br /&gt;
Christopher Geier&lt;br /&gt;
Faysal Hossain Shezan&lt;br /&gt;
Nathaniel Grevatt&lt;br /&gt;
Nishant Jha&lt;br /&gt;
Tanmoy Sen&lt;/p&gt;

&lt;p&gt;The teams will probably adjust a bit over the next week as some people
who registered for the class didn&amp;rsquo;t submit a survey, and others may be
joining the class late, etc., but otherwise we will plan to keep with
these teams through spring break and possible re-arrange things after
that.  You can, of course, rename your team and develop your own
adversarial (but tasteful) team icon.&lt;/p&gt;

&lt;p&gt;Each week, one team will be responsible for leading the class, one
team for blogging (writing a summary of the class), and one team for
food. Everyone in the class is expected to do the preparation for each
meeting, which will usually involve reading a few papers (but may
involve other activities, at the design of the leading team).  See
&lt;a href=&#34;https://secml.github.io/teams/&#34;&gt;https://secml.github.io/teams/&lt;/a&gt; for
the description of team responsibilities.  For the first week, Team
Bus will be the leading team, Team Gibbon will be the feeding team,
and Team Panda will be the blogging team.  I have created a slack
channel for each team, and you should have an invitation to join
it. The immediate tasks for each team are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Team Bus: decide on a leader who will be responsible for coordinating your team for this week, come up with some ideas what you would like to do for Friday&amp;rsquo;s meeting, make sure there are some people who can come to my office hours on Monday (2:30pm) - if not enough people can come then, arrange an alternative time with me, and plan an exciting and worthwhile seminar for Friday.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Team Gibbon: you are the feeding team for this week, so should select 2-3 people to take care of this. One of you should pick up a credit card from me on Thursday to pay for the food. You will be leading the seminar on 2 Feb, so should determine a leader for this, and once Team Bus posts the topic and paper for this week (which should happen on Monday), should start planning your topic. We should meet briefly after Friday&amp;rsquo;s seminar, to discuss your plans, and then a longer meeting the following Monday.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Team Panda: you are the blogging team for this week, so should get familiar with &lt;a href=&#34;https://secml.github.io/blogging/&#34;&gt;https://secml.github.io/blogging/&lt;/a&gt;. You will be leading the seminar or 9 Feb, so should start thinking of topics you would like to lead. You should identify leaders for blogging, feeding, and leading to be ready for the next 3 weeks.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;&amp;mdash; Dave&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Starting Seminar</title>
      <link>https://secml.github.io/starting-seminar/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/starting-seminar/</guid>
      <description>&lt;p&gt;Our first seminar meeting will be Friday, January 26 (not this Friday, which would normally be the first class day, since I will be getting back from California too late to meet this week). Meetings will be Fridays in Rice 032, 9:30am-noon.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re missing the normal first meeting, I want to do as much of the organizational stuff this week to be able to have a substantive first meeting next week. This means we will group students into teams, have a team assigned to lead the first class, and announced topic and readings by next Tuesday (Jan 23).&lt;/p&gt;

&lt;p&gt;To enable this, everyone interested in participating in the class in any way should:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Submit this form by Wednesday (Jan 24): &lt;a href=&#34;https://goo.gl/forms/FqSmUNHYBPbaKsg72&#34;&gt;https://goo.gl/forms/FqSmUNHYBPbaKsg72&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Read the &lt;a href=&#34;https://secml.github.io/syllabus/&#34;&gt;course syllabus&lt;/a&gt;, and follow the directions there to join the seminar slack group.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will work out the teams based on the form submissions, and set up the schedule for leading classes/blogging. I will have office hours on Monday (Jan 22) at 2:30pm. People from the first presenting team will be expect to meet with me then (not necessarily everyone but at least some of the team), and anyone with questions about the seminar is also welcome to come by. (If this time works for the class, we&amp;rsquo;ll keep this as my regular office hours time for the semester.)&lt;/p&gt;

&lt;p&gt;Sorry for missing the first class, but I think we&amp;rsquo;ll be able to manage most of what we would have done the first day electronically, and be able to have an effective first meeting on Jan 26.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syllabus Posted</title>
      <link>https://secml.github.io/syllabusposted/</link>
      <pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/syllabusposted/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://secml.github.io/syllabus&#34;&gt;Syllabus&lt;/a&gt; is now posted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>https://secml.github.io/welcome/</link>
      <pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/welcome/</guid>
      <description>&lt;p&gt;This graduate-level special topics course will be offered in Spring
2018. Meetings will be &lt;strong&gt;Fridays, 9:30-noon&lt;/strong&gt; in Rice Hall 032. More
information will be posted here soon, but the seminar format will be
roughly similar to what we used to &lt;a
href=&#34;https://tlseminar.github.io&#34;&gt;TLSeminar&lt;/a&gt; last Spring.&lt;/p&gt;

&lt;p&gt;This seminar will focus on understanding the risks adversaries pose to
machine learning systems, and how to design more robust machine
learning systems to mitigate those risks.&lt;/p&gt;

&lt;p&gt;The seminar is open to ambitious undergraduate students (with
instructor permission), and to graduate students interested in
research in adversarial machine learning, privacy-preserving machine
learning, fairness and transparency in machine learning, and other
related topics.  Previous background in machine learning and security
is beneficial, but not required so long as you are willing and able to
learn some foundational materials on your own.  &lt;/p&gt; &lt;p&gt; For more
information, contact &lt;A href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David
Evans&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://secml.github.io/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/resources/</guid>
      <description>

&lt;h1 id=&#34;resources-for-adversarial-machine-learning-research&#34;&gt;Resources for Adversarial Machine Learning Research&lt;/h1&gt;

&lt;h2 id=&#34;adversarial-machine-learning-toolkits&#34;&gt;Adversarial Machine Learning Toolkits&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;evademl.org/zoo&#34;&gt;EvadeML-Zoo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/cleverhans&#34;&gt;cleverhans&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;machine-learning-systems&#34;&gt;Machine Learning Systems&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/Detectron&#34;&gt;Detectron&lt;/a&gt; - Facebook&amp;rsquo;s research platform for object detection research (including RetinaNet)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://secml.github.io/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/schedule/</guid>
      <description>&lt;p&gt;See &lt;a href=&#34;https://secml.github.io/teams&#34;&gt;Teams&lt;/a&gt; for the class teams and responsibilities.&lt;/p&gt;

&lt;table&gt;
&lt;tr bgcolor=&#34;#CCC&#34;&gt;&lt;td style=&#34;text-align:center&#34; width=&#34;20%&#34;&gt;&lt;b&gt;Date&lt;/b&gt;&lt;/td&gt;&lt;td width=&#34;30%&#34; style=&#34;text-align:center&#34;&gt;&lt;b&gt;Topic&lt;/b&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; width=&#34;10%&#34;&gt;&lt;b&gt;Bus&lt;/b&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; width=&#34;10%&#34;&gt;&lt;b&gt;Gibbon&lt;/b&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; width=10%&gt;&lt;b&gt;Panda&lt;/b&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; width=12%&gt;&lt;b&gt;&lt;font size=&#34;-1&#34;&gt;Nematode&lt;/font&gt;&lt;/b&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class1&#34;&gt;Class 1&lt;/a&gt; (26 Jan)&lt;/td&gt;&lt;td&gt;Intro to Adversarial ML&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class2&#34;&gt;Class 2&lt;/a&gt; (2 Feb)&lt;/td&gt;&lt;td&gt;Privacy and ML&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class3&#34;&gt;Class 3&lt;/a&gt; (9 Feb)&lt;/td&gt;&lt;td&gt;Adversarial Examples&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class4&#34;&gt;Class 4&lt;/a&gt; (16 Feb)&lt;/td&gt;&lt;td&gt;Privacy in Action&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class5&#34;&gt;Class 5&lt;/a&gt; (23 Feb)&lt;/td&gt;&lt;td&gt;Audio, Text, Faces&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class6&#34;&gt;Class 6&lt;/a&gt; (2 Mar)&lt;/td&gt;&lt;td&gt;Robustness&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=6 bgcolor=&#34;#66EEAA&#34; style=&#34;text-align:center&#34;&gt;Spring Break&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class7&#34;&gt;Class 7&lt;/a&gt; (16 Mar)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class8&#34;&gt;Class 8&lt;/a&gt; (23 Mar)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class9&#34;&gt;Class 9&lt;/a&gt; (30 Mar)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class10&#34;&gt;Class 10&lt;/a&gt; (6 Apr)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class11&#34;&gt;Class 11&lt;/a&gt; (13 Apr)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class12&#34;&gt;Class 12&lt;/a&gt; (20 Apr)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34; &gt;Food&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCC&#34;&gt;&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#DEF&#34;&gt;Blog&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  bgcolor=&#34;#CCDD55&#34;&gt;Lead&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://secml.github.io/class13&#34;&gt;Class 13&lt;/a&gt; (26 Apr)&lt;/td&gt;&lt;td style=&#34;text-align:center&#34;  colspan=5 bgcolor=&#34;#FF3&#34; style=&#34;text-align:center&#34;&gt;Mini-Conference&lt;/td&gt;&lt;/tr&gt;

&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://secml.github.io/syllabus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/syllabus/</guid>
      <description>

&lt;h2 id=&#34;syllabus&#34;&gt;Syllabus&lt;/h2&gt;

&lt;h3 id=&#34;cs6501-security-and-privacy-of-machine-learning&#34;&gt;&lt;strong&gt;cs6501: Security and Privacy of Machine Learning&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;University of Virginia, Spring 2018&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; Fridays, 9:30AM - noon, Rice Hall 032&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Course Objective.&lt;/strong&gt; This seminar will focus on understanding the
  risks adversaries pose to machine learning systems, and how to
  design more robust machine learning systems to mitigate those risks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected Background:&lt;/strong&gt; Previous background in machine learning and
security is beneficial, but not required so long as you are willing
and able to learn some foundational materials on your own. Most
students in the seminar should have either strong background in
machine learning, or strong background in security and privacy, but it
is not expected that most students have extensive background in both
areas. The seminar is open to ambitious undergraduate students (with
instructor permission), and to graduate students interested in
research in adversarial machine learning, privacy-preserving machine
learning, fairness and transparency in machine learning, and other
related topics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Coordinator:&lt;/strong&gt; &lt;a href=&#34;http://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt;
  (evans@virginia.edu). My
  &lt;a href=&#34;http://www.cs.virginia.edu/evans/office&#34;&gt;office&lt;/a&gt; is Rice 507.&lt;/p&gt;

&lt;h2 id=&#34;course-expectations&#34;&gt;Course Expectations&lt;/h2&gt;

&lt;p&gt;Students in the seminar are expected to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lead discussions on interesting topics during the class meetings.
For each week, there will be a team of students charged with
preparing a topic and leading the discussion, and another team
charged with writing a blog post about the class. Students
responsible for posting the blog summary will be different from the
ones charged with leading the topic discussion, but should work
closely with the leaders on the posted write-up.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Particpate actively in class meetings.  This means being prepared to
contribute by doing the assigned preparation (which will typically
involve reading a few research papers, but may involve other things
also) and thinking about the materials deeply to be able to
contribute well to discussions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Contribute fully to a team that develops a course-long project which
could either be a research project or a systematization of knowledge
project. We will discuss this more in an early class, and form teams
based on interests.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;communications&#34;&gt;Communications&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Course Website:&lt;/strong&gt;
  &lt;a href=&#34;https://secml.github.io/&#34;&gt;&lt;em&gt;https://secml.github.io/&lt;/em&gt;&lt;/a&gt;.  All
  course materials will be posted on the course website, and students
  will be expected to provide materials to add to this site.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slack:&lt;/strong&gt;
  &lt;a href=&#34;https://secprivml.slack.com&#34;&gt;&lt;em&gt;https://secprivml.slack.com&lt;/em&gt;&lt;/a&gt;.  We
  will use a slack group for class communications.  You can join using
  any &lt;code&gt;@virginia.edu&lt;/code&gt; email address.  You can also create slack
  channels for your team communications.&lt;/p&gt;

&lt;h2 id=&#34;honor-and-responsibility&#34;&gt;Honor and Responsibility&lt;/h2&gt;

&lt;p&gt;We believe strongly in the value of a &lt;em&gt;community of trust&lt;/em&gt;, and expect
all of the students in this class to contribute to strenghtening and
enhancing that community.  The course will be better for everyone if
everyone can assume everyone else is trustworthy. The course staff
starts with the assumption that all students at the university deserve
to be trusted.&lt;/p&gt;

&lt;p&gt;In this course, we will be learning about and exploring some
vulnerabilities that could be used to compromise deployed systems.
&lt;strong&gt;You are trusted to behave responsibility and ethically.&lt;/strong&gt; You may
not attack any system without permission of its owners, and may not
use anything you learn in this class for evil.  If you have any doubts
about whether or not something you want to do is ethical and legal,
you should check with the course instructor before proceeding.&lt;/p&gt;

&lt;h2 id=&#34;area-requirements&#34;&gt;Area Requirements&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Note for CS Graduate Students.&lt;/strong&gt; This course is mislisted in SIS
(indeed, it is a &amp;ldquo;bug&amp;rdquo; in the setup of SIS that cannot be overcome
that requires all grad courses to be assigned areas) as counting for
the &amp;ldquo;Software Systems&amp;rdquo; and &amp;ldquo;Theory&amp;rdquo; area requirements.  As per the
actual rules in the Graduate Handbook, a cs6501 seminar course does
not a priori count for any particular areas.  It may be possible to
count it for any area, but it would be up to you to make the case to
your committee that it should count for a given area. In most cases,
this will depend a lot on what you individually do in the class - for
example, you could select presentation topics and a topic for you
project that would make a strong case for counting it for the &amp;ldquo;Theory&amp;rdquo;
area, but someone else who does a systems-focused project would be
able to count it for a different area. I can help provide guidance on
this, but it is ultimately up to your committee to decide if a course
counts for a particular area requirement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://secml.github.io/teams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://secml.github.io/teams/</guid>
      <description>

&lt;h2 id=&#34;teams&#34;&gt;Teams&lt;/h2&gt;

&lt;table&gt;
&lt;tr bgcolor=&#34;#CCC&#34;&gt;&lt;td align=&#34;center&#34;&gt;&lt;b&gt;Team Bus&lt;/b&gt;&lt;/td&gt;&lt;td align=&#34;center&#34;&gt;&lt;b&gt;Team Gibbon&lt;/b&gt;&lt;/td&gt;&lt;td align=&#34;center&#34;&gt;&lt;b&gt;Team Panda&lt;/b&gt;&lt;/td&gt;&lt;td align=&#34;center&#34;&gt;&lt;b&gt;Team Nematode&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
Anant Kharkar&lt;br&gt;
Atallah Hezbor  &lt;br&gt;
Bhuvanesh Murali&lt;br&gt;
Mainuddin Jonas&lt;br&gt;  
Weilin Xu&lt;br&gt;
&lt;/td&gt;
&lt;td&gt;
Aditi Narvekar&lt;br&gt;  
Austin Chen  &lt;br&gt;
Ethan Lowman  &lt;br&gt;
Jin Ding  &lt;br&gt;
Suya
&lt;/td&gt;
&lt;td&gt;
Christopher Geier  &lt;br&gt;
Faysal Hossain Shezan  &lt;br&gt;
Helen Simecek&lt;br&gt;
Lawrence Hook&lt;br&gt;
Nishant Jha  &lt;br&gt;
&lt;/td&gt;
&lt;td&gt;
Bargav Jayaraman&lt;br&gt;  
Guy &#34;Jack&#34; Verrier&lt;br&gt;  
Joshua Holtzman  &lt;br&gt;
Max Naylor&lt;br&gt;
Nan Yang&lt;br&gt;
Tanmoy Sen  
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;/table&gt;&lt;/p&gt;

&lt;h1 id=&#34;responsibilities&#34;&gt;Responsibilities&lt;/h1&gt;

&lt;p&gt;For each week (except for project proposal and presentation weeks),
one team will be responsible for Leading the class, one team for
writing a Blog post on the class topic, and one team for arranging
food.  See the &lt;a href=&#34;https://secml.github.io/schedule&#34;&gt;Schedule&lt;/a&gt; for team responsibilities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leading Team.&lt;/strong&gt;  The team responsible for leading a class should:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Two weeks before the scheduled class, meet briefly with me (Dave) to
discuss plan for the class. You should decide on a team leader for
this class, who will be the one responsible for making sure everyone
on the team knows what they are doing and coordinating the team&amp;rsquo;s
efforts.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The Monday the week of the class, at least a few representatives
from the team should come to my office hours to discuss the plan for
the class.  You should come prepared to this meeting with suggested
papers and ideas about how to present them.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;No later than the Monday before class, send out to the class Slack group the preparation materials for the class.  This can include links to papers to read, but could also include exercises to do or software to install and experiment with, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Day of class: lead an interesting, engaging, and illuminating class! This is a 2.5 hour class, so it can&amp;rsquo;t just be a series of unconnected, dull presentations. You need to think of things to do in class to make it more worthwhile and engaging.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;After class: help the Blogging team by providing them with your materials, answering their questions, and reviewing their write-up.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Blogging Team.&lt;/strong&gt; The team responsible for blogging a class should:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The week before the scheduled class, develop a team plan for how to
manage the blogging. One team member should be designated the team
leader for the blogging, and post on slack so we know who is
responsible. The blogging leader is responsible for making sure the
team is well coordinated and everyone knows what they are doing and
follows through on this.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During class, participate actively in the class, and take detailed
notes (this can be distributed among the team).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By the Tuesday following class, have a draft of the blog post ready,
and share it with the rest of the class (including the leading team
and coordinators) for comments. Details on how to prepare the blog post are on the &lt;a href=&#34;https://secml.github.io/blogging&#34;&gt;Blogging Mechanics&lt;/a&gt; page.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By the next Friday (one week after the class), have a final version
of the blog post ready to add to the course site.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Feeding Team.&lt;/strong&gt; The team responsible for food should:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Plan among yourselves what food to bring and who is responsible.  If
you want to use my credit card to buy food, borrow it. You can stop
by Thursday afternoon to pick it up from me.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bring something yummy and something with caffiene, but not too messy
or disruptive, to class. A simple choice is to get a coffee
container and bagels from the bagel shop in Rice Hall (but make sure
to get the order in early enough to be ready before class). More
adventurous choices are encouraged.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure to clean up the room at the end of class. If we get caught
leaving a mess, we probably will not be allowed to have food
anymore.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Save the receipts to get reimbursed, and take care of the
reimbursement. This is easiest if you just borrow my credit card and
then all you need to do is send me an image of the receipt (or hand
me a physical receipt).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>